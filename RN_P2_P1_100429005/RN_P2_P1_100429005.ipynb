{"cells":[{"cell_type":"markdown","metadata":{"id":"TDJtCWTTw4gk"},"source":["# Practica 2\n","\n","Autor: Lucas Gallego Bravo\n","\n","NIA: 100429005"]},{"cell_type":"markdown","metadata":{"id":"vFWRnnmcxTmh"},"source":["# Imports"]},{"cell_type":"markdown","metadata":{"id":"B9gVsXbMxVYj"},"source":["Antes de comenzar a implementar el código de la práctica, debemos realizar los imports necesarios para el correcto funcionamiento de la misma."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13612,"status":"ok","timestamp":1702672473333,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"MUa3rGvHvlpr","outputId":"c6a1cd33-4adc-4238-aae0-8fd96acd4264"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"]}],"source":["!pip3 install numpy\n","!pip3 install pandas\n","\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.utils import class_weight\n","from numpy.random import seed\n","from tensorflow.keras.utils import set_random_seed\n","from sklearn.preprocessing import LabelBinarizer\n","from keras.callbacks import ModelCheckpoint\n","from keras.losses import binary_crossentropy\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix,mean_squared_error\n","\n","#from tensorflow.keras import datasets, layers, models\n","#import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"bEVeKNQ_xgmA"},"source":["# Preprocesamiento de los datos"]},{"cell_type":"markdown","metadata":{"id":"MsXI4wT3xk7u"},"source":["Una vez realizados los imports necesarios, podemos empezar a realizar el preprocesamiento de los datos. Lo cual ayudara a nuestro PM en la clasificación."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":732,"status":"ok","timestamp":1702660625647,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"lSDe3YWlxlNE","outputId":"9cd6589b-d8c3-447e-ef79-f78a29a0f0a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset(data):  (29151, 23)\n"]}],"source":["# Lo primero que debemos realizar es la apertura de nuestro dataset:\n","# El path y file_ se pueden cambiar para acomodarse al path y fichero necesarios\n","path= \"/content/\"\n","file_ = \"dataset.csv\"\n","dataset = pd.read_csv(path + file_, header='infer', delimiter=',')\n","\n","# Mediante estos prints podemos comprobar la dimensionalidad de nuestros datos y target y asi comprobar que tienen las dimensiones correctas:\n","print('Dataset(data): ',dataset.shape)"]},{"cell_type":"markdown","metadata":{"id":"EBzjFhQh14iI"},"source":["## Elección de las variables"]},{"cell_type":"markdown","metadata":{"id":"ncTLzxD-4-ZF"},"source":["Primero se van a eliminar todas las columnas std_, ya que solo nos interesa la media de los datos dados. También se eliminaran las columnas 'track_id' y 'user_id' al considerarse que son datos irrelevantes para la clasificación."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":480,"status":"ok","timestamp":1702660627647,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"7cl91ZjR5FsA","outputId":"a07b4c76-8c55-40b5-b3fc-8739e743265e"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-d15c3859-df71-4fc6-b295-59abbb511f27\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean_delta_times</th>\n","      <th>mean_hours</th>\n","      <th>mean_distances</th>\n","      <th>mean_velocities</th>\n","      <th>mean_accelerations</th>\n","      <th>mean_headings</th>\n","      <th>mean_heading_changes</th>\n","      <th>mean_heading_change_rates</th>\n","      <th>mean_stops</th>\n","      <th>mean_turnings</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.954975</td>\n","      <td>4.0</td>\n","      <td>7.370991</td>\n","      <td>1.475115</td>\n","      <td>0.000224</td>\n","      <td>141.946162</td>\n","      <td>2.357923</td>\n","      <td>0.297172</td>\n","      <td>0.079146</td>\n","      <td>0.304824</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.890226</td>\n","      <td>4.0</td>\n","      <td>4.358587</td>\n","      <td>0.833067</td>\n","      <td>0.004301</td>\n","      <td>154.832162</td>\n","      <td>5.153850</td>\n","      <td>6.498869</td>\n","      <td>0.444698</td>\n","      <td>0.613568</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.000000</td>\n","      <td>1.0</td>\n","      <td>2.298434</td>\n","      <td>1.149217</td>\n","      <td>0.002897</td>\n","      <td>90.287825</td>\n","      <td>0.338599</td>\n","      <td>0.169300</td>\n","      <td>0.835000</td>\n","      <td>0.355000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.000000</td>\n","      <td>1.0</td>\n","      <td>2.135659</td>\n","      <td>1.067830</td>\n","      <td>0.010021</td>\n","      <td>85.249516</td>\n","      <td>1.585766</td>\n","      <td>0.792883</td>\n","      <td>0.850603</td>\n","      <td>0.339296</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.000000</td>\n","      <td>1.0</td>\n","      <td>0.321824</td>\n","      <td>0.160912</td>\n","      <td>0.003865</td>\n","      <td>91.970513</td>\n","      <td>0.810080</td>\n","      <td>0.405040</td>\n","      <td>0.988367</td>\n","      <td>0.494121</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>29146</th>\n","      <td>5.616181</td>\n","      <td>1.0</td>\n","      <td>77.617958</td>\n","      <td>12.020115</td>\n","      <td>0.017929</td>\n","      <td>190.504070</td>\n","      <td>1.334386</td>\n","      <td>0.308397</td>\n","      <td>0.020101</td>\n","      <td>0.055276</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>29147</th>\n","      <td>4.625779</td>\n","      <td>1.0</td>\n","      <td>36.301740</td>\n","      <td>6.751808</td>\n","      <td>0.028833</td>\n","      <td>161.341672</td>\n","      <td>2.354026</td>\n","      <td>-0.320778</td>\n","      <td>0.275427</td>\n","      <td>0.260126</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>29148</th>\n","      <td>21.353794</td>\n","      <td>12.0</td>\n","      <td>53.414140</td>\n","      <td>2.565517</td>\n","      <td>0.006307</td>\n","      <td>163.727055</td>\n","      <td>1.158802</td>\n","      <td>0.400712</td>\n","      <td>0.000000</td>\n","      <td>0.308442</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>29149</th>\n","      <td>3.427136</td>\n","      <td>12.0</td>\n","      <td>7.600539</td>\n","      <td>3.523266</td>\n","      <td>0.048029</td>\n","      <td>112.152390</td>\n","      <td>6.595061</td>\n","      <td>2.322378</td>\n","      <td>0.456131</td>\n","      <td>0.292538</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>29150</th>\n","      <td>3.718844</td>\n","      <td>12.0</td>\n","      <td>8.267408</td>\n","      <td>3.945757</td>\n","      <td>0.230183</td>\n","      <td>64.367613</td>\n","      <td>-0.727783</td>\n","      <td>-1.214783</td>\n","      <td>0.426382</td>\n","      <td>0.331583</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>29151 rows × 11 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d15c3859-df71-4fc6-b295-59abbb511f27')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d15c3859-df71-4fc6-b295-59abbb511f27 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d15c3859-df71-4fc6-b295-59abbb511f27');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-fcb4735f-db59-4c1d-8de1-d1999efd89db\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fcb4735f-db59-4c1d-8de1-d1999efd89db')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-fcb4735f-db59-4c1d-8de1-d1999efd89db button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["       mean_delta_times  mean_hours  mean_distances  mean_velocities  \\\n","0              4.954975         4.0        7.370991         1.475115   \n","1              4.890226         4.0        4.358587         0.833067   \n","2              2.000000         1.0        2.298434         1.149217   \n","3              2.000000         1.0        2.135659         1.067830   \n","4              2.000000         1.0        0.321824         0.160912   \n","...                 ...         ...             ...              ...   \n","29146          5.616181         1.0       77.617958        12.020115   \n","29147          4.625779         1.0       36.301740         6.751808   \n","29148         21.353794        12.0       53.414140         2.565517   \n","29149          3.427136        12.0        7.600539         3.523266   \n","29150          3.718844        12.0        8.267408         3.945757   \n","\n","       mean_accelerations  mean_headings  mean_heading_changes  \\\n","0                0.000224     141.946162              2.357923   \n","1                0.004301     154.832162              5.153850   \n","2                0.002897      90.287825              0.338599   \n","3                0.010021      85.249516              1.585766   \n","4                0.003865      91.970513              0.810080   \n","...                   ...            ...                   ...   \n","29146            0.017929     190.504070              1.334386   \n","29147            0.028833     161.341672              2.354026   \n","29148            0.006307     163.727055              1.158802   \n","29149            0.048029     112.152390              6.595061   \n","29150            0.230183      64.367613             -0.727783   \n","\n","       mean_heading_change_rates  mean_stops  mean_turnings  label  \n","0                       0.297172    0.079146       0.304824      0  \n","1                       6.498869    0.444698       0.613568      0  \n","2                       0.169300    0.835000       0.355000      0  \n","3                       0.792883    0.850603       0.339296      0  \n","4                       0.405040    0.988367       0.494121      0  \n","...                          ...         ...            ...    ...  \n","29146                   0.308397    0.020101       0.055276      3  \n","29147                  -0.320778    0.275427       0.260126      3  \n","29148                   0.400712    0.000000       0.308442      1  \n","29149                   2.322378    0.456131       0.292538      4  \n","29150                  -1.214783    0.426382       0.331583      4  \n","\n","[29151 rows x 11 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Ahora debemos seleccionar que columnas consideramos relevantes en nuestro dataset, dicho esto, primero borramos las columnas std_:\n","\n","dataFrame = dataset.drop(columns=[\"std_delta_times\", \"std_hours\", \"std_distances\", \"std_velocities\", \"std_accelerations\",\"std_headings\",\"std_heading_changes\",\"std_heading_change_rates\",\"std_stops\",\"std_turnings\"] )\n","\n","#Una vez borrados los std_, vamos a borrar tambien la columna track_id, ya que al tener la columna user_id se cree que track_id es innecesaria:\n","\n","dataFrame = dataFrame.drop(columns=[\"track_id\",\"user_id\"])\n","\n","dataFrame"]},{"cell_type":"markdown","metadata":{"id":"f4xQAAbY_x4T"},"source":["## Normalización de los datos\n","\n","Una vez tenemos los datos limpios, podemos normalizar los datos en el rango de valores [0,1], lo hacemos solo de las 10 primeras columnas ya que la última (target) no debe ser modificada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IaBZVyNG_yK0"},"outputs":[],"source":["maximos_col = dataFrame.max()\n","\n","minimos_col = dataFrame.min()\n","\n","i=0\n","j=0\n","\n","while j < 10:\n","\n","  min_column = maximos_col[j]\n","  max_column = minimos_col[j]\n","\n","  var1 = dataFrame.iloc[i,j] - min_column\n","  var2 = max_column - min_column\n","\n","  norm_val = var1 / var2\n","\n","  dataFrame.iloc[i,j] = norm_val\n","\n","  i += 1\n","\n","  if i == 29151 :\n","\n","    i = 0\n","    j += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":366,"status":"ok","timestamp":1702660684631,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"axVG1IP9fCX-","outputId":"9ba0e4f2-f6e8-48ce-9aa4-1ff1ddab3cf9"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-44693a0a-e004-4b17-9c72-531e828d73f1\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean_delta_times</th>\n","      <th>mean_hours</th>\n","      <th>mean_distances</th>\n","      <th>mean_velocities</th>\n","      <th>mean_accelerations</th>\n","      <th>mean_headings</th>\n","      <th>mean_heading_changes</th>\n","      <th>mean_heading_change_rates</th>\n","      <th>mean_stops</th>\n","      <th>mean_turnings</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.974355</td>\n","      <td>0.826087</td>\n","      <td>0.997343</td>\n","      <td>0.967323</td>\n","      <td>0.596435</td>\n","      <td>0.604068</td>\n","      <td>0.639166</td>\n","      <td>0.566907</td>\n","      <td>0.920457</td>\n","      <td>0.681148</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.974775</td>\n","      <td>0.826087</td>\n","      <td>0.998429</td>\n","      <td>0.981546</td>\n","      <td>0.594602</td>\n","      <td>0.568125</td>\n","      <td>0.590496</td>\n","      <td>0.481978</td>\n","      <td>0.553067</td>\n","      <td>0.358196</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.993516</td>\n","      <td>0.956522</td>\n","      <td>0.999171</td>\n","      <td>0.974542</td>\n","      <td>0.595233</td>\n","      <td>0.748159</td>\n","      <td>0.674318</td>\n","      <td>0.568658</td>\n","      <td>0.160804</td>\n","      <td>0.628663</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.993516</td>\n","      <td>0.956522</td>\n","      <td>0.999230</td>\n","      <td>0.976345</td>\n","      <td>0.592029</td>\n","      <td>0.762213</td>\n","      <td>0.652608</td>\n","      <td>0.560118</td>\n","      <td>0.145123</td>\n","      <td>0.645089</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.993516</td>\n","      <td>0.956522</td>\n","      <td>0.999884</td>\n","      <td>0.996435</td>\n","      <td>0.594798</td>\n","      <td>0.743466</td>\n","      <td>0.666110</td>\n","      <td>0.565430</td>\n","      <td>0.006666</td>\n","      <td>0.483140</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>29146</th>\n","      <td>0.970068</td>\n","      <td>0.956522</td>\n","      <td>0.972016</td>\n","      <td>0.733728</td>\n","      <td>0.588473</td>\n","      <td>0.468625</td>\n","      <td>0.656984</td>\n","      <td>0.566753</td>\n","      <td>0.979798</td>\n","      <td>0.942180</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>29147</th>\n","      <td>0.976490</td>\n","      <td>0.956522</td>\n","      <td>0.986912</td>\n","      <td>0.850432</td>\n","      <td>0.583569</td>\n","      <td>0.549968</td>\n","      <td>0.639234</td>\n","      <td>0.575369</td>\n","      <td>0.723189</td>\n","      <td>0.727903</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>29148</th>\n","      <td>0.868024</td>\n","      <td>0.478261</td>\n","      <td>0.980742</td>\n","      <td>0.943168</td>\n","      <td>0.593700</td>\n","      <td>0.543314</td>\n","      <td>0.660040</td>\n","      <td>0.565489</td>\n","      <td>1.000000</td>\n","      <td>0.677363</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>29149</th>\n","      <td>0.984262</td>\n","      <td>0.478261</td>\n","      <td>0.997260</td>\n","      <td>0.921952</td>\n","      <td>0.574936</td>\n","      <td>0.687172</td>\n","      <td>0.565408</td>\n","      <td>0.539173</td>\n","      <td>0.541577</td>\n","      <td>0.694000</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>29150</th>\n","      <td>0.982371</td>\n","      <td>0.478261</td>\n","      <td>0.997019</td>\n","      <td>0.912593</td>\n","      <td>0.493017</td>\n","      <td>0.820459</td>\n","      <td>0.692881</td>\n","      <td>0.587612</td>\n","      <td>0.571475</td>\n","      <td>0.653158</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>29151 rows × 11 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44693a0a-e004-4b17-9c72-531e828d73f1')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-44693a0a-e004-4b17-9c72-531e828d73f1 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-44693a0a-e004-4b17-9c72-531e828d73f1');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e3d0ba65-47cb-46b3-a8ea-a82493e17807\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3d0ba65-47cb-46b3-a8ea-a82493e17807')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e3d0ba65-47cb-46b3-a8ea-a82493e17807 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["       mean_delta_times  mean_hours  mean_distances  mean_velocities  \\\n","0              0.974355    0.826087        0.997343         0.967323   \n","1              0.974775    0.826087        0.998429         0.981546   \n","2              0.993516    0.956522        0.999171         0.974542   \n","3              0.993516    0.956522        0.999230         0.976345   \n","4              0.993516    0.956522        0.999884         0.996435   \n","...                 ...         ...             ...              ...   \n","29146          0.970068    0.956522        0.972016         0.733728   \n","29147          0.976490    0.956522        0.986912         0.850432   \n","29148          0.868024    0.478261        0.980742         0.943168   \n","29149          0.984262    0.478261        0.997260         0.921952   \n","29150          0.982371    0.478261        0.997019         0.912593   \n","\n","       mean_accelerations  mean_headings  mean_heading_changes  \\\n","0                0.596435       0.604068              0.639166   \n","1                0.594602       0.568125              0.590496   \n","2                0.595233       0.748159              0.674318   \n","3                0.592029       0.762213              0.652608   \n","4                0.594798       0.743466              0.666110   \n","...                   ...            ...                   ...   \n","29146            0.588473       0.468625              0.656984   \n","29147            0.583569       0.549968              0.639234   \n","29148            0.593700       0.543314              0.660040   \n","29149            0.574936       0.687172              0.565408   \n","29150            0.493017       0.820459              0.692881   \n","\n","       mean_heading_change_rates  mean_stops  mean_turnings  label  \n","0                       0.566907    0.920457       0.681148      0  \n","1                       0.481978    0.553067       0.358196      0  \n","2                       0.568658    0.160804       0.628663      0  \n","3                       0.560118    0.145123       0.645089      0  \n","4                       0.565430    0.006666       0.483140      0  \n","...                          ...         ...            ...    ...  \n","29146                   0.566753    0.979798       0.942180      3  \n","29147                   0.575369    0.723189       0.727903      3  \n","29148                   0.565489    1.000000       0.677363      1  \n","29149                   0.539173    0.541577       0.694000      4  \n","29150                   0.587612    0.571475       0.653158      4  \n","\n","[29151 rows x 11 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dataFrame"]},{"cell_type":"markdown","metadata":{"id":"ABELQCFcAaR0"},"source":["## División entre train y test:\n","Dividimos nuestros datos entre train (2/3) y test (1/3), además, los transformamos en unos .csv los cuales descargaremos para su uso mas adelante."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCxhl9_sAjOr"},"outputs":[],"source":["train = dataFrame[0:19434]\n","\n","test = dataFrame[19434:]\n","\n","train.to_csv('train.csv',index=False)\n","\n","test.to_csv('test.csv',index=False)"]},{"cell_type":"markdown","metadata":{"id":"JY06MoBrVncR"},"source":["# PARTE 1"]},{"cell_type":"markdown","metadata":{"id":"3kSrAcEoxnxG"},"source":["## Apertura de train.csv y test.csv:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702663571951,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"xNswvUBxxoBr","outputId":"635ad177-eea2-4dfb-fdd3-6db67122fdaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape X_train:  (19434, 10)\n","Shape y_train:  (19434, 1)\n","Shape X_test:  (9717, 10)\n","Shape y_test:  (9717, 1)\n"]}],"source":["path= \"/content/\"\n","file_ = \"train.csv\"\n","entrenamiento = pd.read_csv(path + file_, header='infer', delimiter=',')\n","\n","file_ = \"test.csv\"\n","test = pd.read_csv(path + file_, header='infer', delimiter=',')\n","\n","X_train = entrenamiento.iloc[: , :-1]\n","y_train = entrenamiento.iloc[:,-1:]\n","\n","X_test = test.iloc[: , :-1]\n","y_test = test.iloc[:,-1:]\n","\n","print(\"Shape X_train: \", X_train.shape)\n","print(\"Shape y_train: \", y_train.shape)\n","print(\"Shape X_test: \", X_test.shape)\n","print(\"Shape y_test: \", y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"2UV5PVYizqgC"},"source":["## Selección de una semilla:\n","\n","Para que todos los procesos sean replicables aun haciendo uso de variables aleatorias, definimos una semilla."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"asGAvtQqzqOt"},"outputs":[],"source":["semilla=72 # segun la semilla, el futuro aleatorio funcionara siempre igual\n","seed(semilla)\n","set_random_seed(semilla)"]},{"cell_type":"markdown","metadata":{"id":"07B3_UUkw2wJ"},"source":["## One-Hot-Encoding:\n","\n","Una vez tenemos la división y normalización de nuestros datos, deberemos realizar one-hot-encoding a nuestra salida (target)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kGeGY7CxO8z"},"outputs":[],"source":["encoder = LabelBinarizer()\n","\n","y_train_encoded = encoder.fit_transform(y_train)\n","y_test_encoded = encoder.fit_transform(y_test)"]},{"cell_type":"markdown","metadata":{"id":"FBhQp1ZifGzW"},"source":["Una vez tenemos los datos de entrenamiento preparados, debemos utilizarlos para sacar el conjunto de validación, el cual corresponde al 20% de los datos de entrenamiento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KHSBs81fDjT"},"outputs":[],"source":["#Extraer del conjunto de train el conjunto de validación\n","X_train2, X_Validation, y_train_encoded2, y_Validation = train_test_split(X_train, y_train_encoded, stratify=y_train_encoded, test_size=0.2)"]},{"cell_type":"markdown","metadata":{"id":"-YWtSFBT3aa0"},"source":["## Criterio de Parada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRgwTvBxUF32"},"outputs":[],"source":["#OPCIÓN 1\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","early_stopping = EarlyStopping(monitor='val_loss',  # métrica para monitorear\n","                               patience=1,  # número de épocas sin mejora antes de detener el entrenamiento\n","                               restore_best_weights=True,  # restaurar los mejores pesos después de detener\n","                               verbose=1)\n","callbacks_list=[early_stopping]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGpLjwDZVwrH"},"outputs":[],"source":["#OPCIÓN 2\n","path= \"/content/\"\n","checkpoint = ModelCheckpoint(\n","                            'pesos_1.h5',  # nombre del archivo para guardar los pesos\n","                             monitor='val_loss',  # métrica para monitorear (puede ser 'val_loss', 'val_accuracy', etc.)\n","                             save_best_only=True,  # guarda solo si la métrica mejora\n","                             mode='min',  # 'max' si quieres maximizar la métrica, 'min' si quieres minimizar\n","                             save_weights_only=True,  # guarda solo los pesos, no el modelo completo\n","                             verbose=1)\n","callbacks_list = [checkpoint]"]},{"cell_type":"markdown","metadata":{"id":"T7o1c3iCWBcI"},"source":["## Asignación de pesos\n","Para esta parte, he decidido asignar los pesos de manera automática, aunque es posible asignarlos manualmente."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702672473334,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"QztsQGzNZ0_x","outputId":"e8dcd076-0816-4f85-9e6f-0f29c8e8f3e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.5265707  1.07442985 0.88385446 1.32880342 3.49370787]\n","{0: 0.5265707027942421, 1: 1.0744298548721494, 2: 0.8838544627629334, 3: 1.3288034188034188, 4: 3.493707865168539}\n"]}],"source":["# Convertir las etiquetas one-hot de nuevo a un formato de una sola columna\n","y_train_single_column = np.argmax(y_train_encoded2, axis=1)\n","\n","\n","# CALCULACIÓN DE PESOS DE CLASE AUTOMÁTICOS\n","pesos = class_weight.compute_class_weight(\n","    'balanced',\n","    classes=np.unique(y_train_single_column),\n","    y=y_train_single_column\n",")\n","\n","print(pesos)\n","\n","# CREACIÓN DEL DICCIONARIO DE PESOS DE CLASE\n","class_weights = {i: pesos[i] for i in range(len(pesos))}\n","\n","print(class_weights)"]},{"cell_type":"markdown","metadata":{"id":"WU6e7V3JQ1Ka"},"source":["## Definir el modelo de red neuronal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":634,"status":"ok","timestamp":1702672484536,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"lf8sGlmvRbCI","outputId":"280c5d30-3bc3-436e-fd80-fc509bc89db4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_23\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_72 (Dense)            (None, 50)                550       \n","                                                                 \n"," dense_73 (Dense)            (None, 50)                2550      \n","                                                                 \n"," dense_74 (Dense)            (None, 50)                2550      \n","                                                                 \n"," dense_75 (Dense)            (None, 5)                 255       \n","                                                                 \n","=================================================================\n","Total params: 5905 (23.07 KB)\n","Trainable params: 5905 (23.07 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Definir forma de la entrada de la red\n","input_shape=(X_train2.shape [1],) # utilizamos los datos de entrenamiento para definir la tupla\n","\n","# Definir la salida de la red, la ultima capa debera tener una neurona por cada clase\n","num_classes = y_train_encoded2.shape[1] # definimos el numero de clases que queremos que tenga la salida\n","\n","#DEFINICIÓN DEL MODELO\n","model = Sequential()\n","\n","# Capa de entrada del modelo\n","model.add(Dense(50, input_shape=input_shape, activation='sigmoid'))\n","\n","#Capas ocultas:\n","model.add(Dense(50, activation='sigmoid'))\n","model.add(Dense(50, activation='sigmoid'))\n","\n","# Capa de salida del modelo\n","model.add(Dense(num_classes, activation='softmax'))\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"Wi903UbQSdnB"},"source":["## Hiperparámetros"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LefVqA4ZSdag"},"outputs":[],"source":["# Hiperparametros\n","lr = 0.00003 # razon de aprendizaje\n","momento=0\n","epochs_H = 200 # numero de ciclos que se quiere realizar en el entrenamiento\n","batch_size_H = 32\n","optimizador=tf.keras.optimizers.SGD(learning_rate=lr, momentum=momento)\n","#lr=1e-3\n","#optimizador=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n","#rho=0.9\n","#lr=0.001\n","#optimizador=tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9),"]},{"cell_type":"markdown","metadata":{"id":"Pnrf4ZVISpil"},"source":["## Compilación del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQ7B-E-TSpV4"},"outputs":[],"source":["#COMPILACIÓN DEL MODELO\n","\n","model.compile(\n","    optimizer = optimizador,\n","    #optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n","    #optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n","    loss='mean_squared_error',\n","    #loss='binary_crossentropy',\n","    metrics=['accuracy','mse']\n",")"]},{"cell_type":"markdown","metadata":{"id":"Cu8uSIMqTRV5"},"source":["## Entrenamiento del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":322605,"status":"ok","timestamp":1702672816514,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"OvN3SvkxTRm_","outputId":"36125f4a-1207-4765-8ba3-f7aa87886e15"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","479/486 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.1504 - mse: 0.1981\n","Epoch 1: val_loss improved from inf to 0.19795, saving model to pesos_1.h5\n","486/486 [==============================] - 5s 8ms/step - loss: 0.1911 - accuracy: 0.1505 - mse: 0.1981 - val_loss: 0.1979 - val_accuracy: 0.1505 - val_mse: 0.1979\n","Epoch 2/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1907 - accuracy: 0.1503 - mse: 0.1978\n","Epoch 2: val_loss improved from 0.19795 to 0.19759, saving model to pesos_1.h5\n","486/486 [==============================] - 6s 12ms/step - loss: 0.1908 - accuracy: 0.1505 - mse: 0.1978 - val_loss: 0.1976 - val_accuracy: 0.1505 - val_mse: 0.1976\n","Epoch 3/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1904 - accuracy: 0.1503 - mse: 0.1974\n","Epoch 3: val_loss improved from 0.19759 to 0.19723, saving model to pesos_1.h5\n","486/486 [==============================] - 4s 7ms/step - loss: 0.1905 - accuracy: 0.1505 - mse: 0.1974 - val_loss: 0.1972 - val_accuracy: 0.1505 - val_mse: 0.1972\n","Epoch 4/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1901 - accuracy: 0.1503 - mse: 0.1971\n","Epoch 4: val_loss improved from 0.19723 to 0.19688, saving model to pesos_1.h5\n","486/486 [==============================] - 3s 7ms/step - loss: 0.1902 - accuracy: 0.1505 - mse: 0.1971 - val_loss: 0.1969 - val_accuracy: 0.1505 - val_mse: 0.1969\n","Epoch 5/200\n","480/486 [============================>.] - ETA: 0s - loss: 0.1898 - accuracy: 0.1505 - mse: 0.1967\n","Epoch 5: val_loss improved from 0.19688 to 0.19653, saving model to pesos_1.h5\n","486/486 [==============================] - 4s 7ms/step - loss: 0.1899 - accuracy: 0.1505 - mse: 0.1967 - val_loss: 0.1965 - val_accuracy: 0.1505 - val_mse: 0.1965\n","Epoch 6/200\n","485/486 [============================>.] - ETA: 0s - loss: 0.1896 - accuracy: 0.1503 - mse: 0.1964\n","Epoch 6: val_loss improved from 0.19653 to 0.19619, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1896 - accuracy: 0.1505 - mse: 0.1964 - val_loss: 0.1962 - val_accuracy: 0.1505 - val_mse: 0.1962\n","Epoch 7/200\n","469/486 [===========================>..] - ETA: 0s - loss: 0.1894 - accuracy: 0.1508 - mse: 0.1960\n","Epoch 7: val_loss improved from 0.19619 to 0.19586, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1893 - accuracy: 0.1505 - mse: 0.1960 - val_loss: 0.1959 - val_accuracy: 0.1505 - val_mse: 0.1959\n","Epoch 8/200\n","468/486 [===========================>..] - ETA: 0s - loss: 0.1892 - accuracy: 0.1506 - mse: 0.1957\n","Epoch 8: val_loss improved from 0.19586 to 0.19554, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1891 - accuracy: 0.1505 - mse: 0.1957 - val_loss: 0.1955 - val_accuracy: 0.1505 - val_mse: 0.1955\n","Epoch 9/200\n","476/486 [============================>.] - ETA: 0s - loss: 0.1888 - accuracy: 0.1503 - mse: 0.1954\n","Epoch 9: val_loss improved from 0.19554 to 0.19521, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.1505 - mse: 0.1954 - val_loss: 0.1952 - val_accuracy: 0.1505 - val_mse: 0.1952\n","Epoch 10/200\n","486/486 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.1505 - mse: 0.1951\n","Epoch 10: val_loss improved from 0.19521 to 0.19490, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1885 - accuracy: 0.1505 - mse: 0.1951 - val_loss: 0.1949 - val_accuracy: 0.1505 - val_mse: 0.1949\n","Epoch 11/200\n","486/486 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.1505 - mse: 0.1948\n","Epoch 11: val_loss improved from 0.19490 to 0.19459, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1883 - accuracy: 0.1505 - mse: 0.1948 - val_loss: 0.1946 - val_accuracy: 0.1505 - val_mse: 0.1946\n","Epoch 12/200\n","484/486 [============================>.] - ETA: 0s - loss: 0.1881 - accuracy: 0.1503 - mse: 0.1945\n","Epoch 12: val_loss improved from 0.19459 to 0.19428, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.1505 - mse: 0.1944 - val_loss: 0.1943 - val_accuracy: 0.1505 - val_mse: 0.1943\n","Epoch 13/200\n","470/486 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.1506 - mse: 0.1941\n","Epoch 13: val_loss improved from 0.19428 to 0.19398, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1878 - accuracy: 0.1505 - mse: 0.1941 - val_loss: 0.1940 - val_accuracy: 0.1505 - val_mse: 0.1940\n","Epoch 14/200\n","466/486 [===========================>..] - ETA: 0s - loss: 0.1877 - accuracy: 0.1507 - mse: 0.1938\n","Epoch 14: val_loss improved from 0.19398 to 0.19369, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1876 - accuracy: 0.1505 - mse: 0.1938 - val_loss: 0.1937 - val_accuracy: 0.1505 - val_mse: 0.1937\n","Epoch 15/200\n","477/486 [============================>.] - ETA: 0s - loss: 0.1873 - accuracy: 0.1504 - mse: 0.1935\n","Epoch 15: val_loss improved from 0.19369 to 0.19340, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1873 - accuracy: 0.1505 - mse: 0.1936 - val_loss: 0.1934 - val_accuracy: 0.1505 - val_mse: 0.1934\n","Epoch 16/200\n","484/486 [============================>.] - ETA: 0s - loss: 0.1871 - accuracy: 0.1503 - mse: 0.1933\n","Epoch 16: val_loss improved from 0.19340 to 0.19311, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1871 - accuracy: 0.1505 - mse: 0.1933 - val_loss: 0.1931 - val_accuracy: 0.1505 - val_mse: 0.1931\n","Epoch 17/200\n","474/486 [============================>.] - ETA: 0s - loss: 0.1868 - accuracy: 0.1505 - mse: 0.1929\n","Epoch 17: val_loss improved from 0.19311 to 0.19283, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1869 - accuracy: 0.1505 - mse: 0.1930 - val_loss: 0.1928 - val_accuracy: 0.1505 - val_mse: 0.1928\n","Epoch 18/200\n","475/486 [============================>.] - ETA: 0s - loss: 0.1867 - accuracy: 0.1504 - mse: 0.1927\n","Epoch 18: val_loss improved from 0.19283 to 0.19255, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1867 - accuracy: 0.1505 - mse: 0.1927 - val_loss: 0.1926 - val_accuracy: 0.1505 - val_mse: 0.1926\n","Epoch 19/200\n","477/486 [============================>.] - ETA: 0s - loss: 0.1864 - accuracy: 0.1504 - mse: 0.1924\n","Epoch 19: val_loss improved from 0.19255 to 0.19228, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1864 - accuracy: 0.1505 - mse: 0.1924 - val_loss: 0.1923 - val_accuracy: 0.1505 - val_mse: 0.1923\n","Epoch 20/200\n","471/486 [============================>.] - ETA: 0s - loss: 0.1862 - accuracy: 0.1507 - mse: 0.1921\n","Epoch 20: val_loss improved from 0.19228 to 0.19201, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1862 - accuracy: 0.1505 - mse: 0.1922 - val_loss: 0.1920 - val_accuracy: 0.1505 - val_mse: 0.1920\n","Epoch 21/200\n","486/486 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.1505 - mse: 0.1919\n","Epoch 21: val_loss improved from 0.19201 to 0.19175, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1860 - accuracy: 0.1505 - mse: 0.1919 - val_loss: 0.1917 - val_accuracy: 0.1505 - val_mse: 0.1917\n","Epoch 22/200\n","467/486 [===========================>..] - ETA: 0s - loss: 0.1859 - accuracy: 0.1506 - mse: 0.1916\n","Epoch 22: val_loss improved from 0.19175 to 0.19149, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1858 - accuracy: 0.1505 - mse: 0.1916 - val_loss: 0.1915 - val_accuracy: 0.1505 - val_mse: 0.1915\n","Epoch 23/200\n","480/486 [============================>.] - ETA: 0s - loss: 0.1856 - accuracy: 0.1505 - mse: 0.1913\n","Epoch 23: val_loss improved from 0.19149 to 0.19123, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.1505 - mse: 0.1914 - val_loss: 0.1912 - val_accuracy: 0.1505 - val_mse: 0.1912\n","Epoch 24/200\n","484/486 [============================>.] - ETA: 0s - loss: 0.1854 - accuracy: 0.1503 - mse: 0.1911\n","Epoch 24: val_loss improved from 0.19123 to 0.19098, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1854 - accuracy: 0.1505 - mse: 0.1911 - val_loss: 0.1910 - val_accuracy: 0.1505 - val_mse: 0.1910\n","Epoch 25/200\n","474/486 [============================>.] - ETA: 0s - loss: 0.1852 - accuracy: 0.1505 - mse: 0.1908\n","Epoch 25: val_loss improved from 0.19098 to 0.19073, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1852 - accuracy: 0.1505 - mse: 0.1909 - val_loss: 0.1907 - val_accuracy: 0.1505 - val_mse: 0.1907\n","Epoch 26/200\n","474/486 [============================>.] - ETA: 0s - loss: 0.1850 - accuracy: 0.1505 - mse: 0.1906\n","Epoch 26: val_loss improved from 0.19073 to 0.19048, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1850 - accuracy: 0.1505 - mse: 0.1906 - val_loss: 0.1905 - val_accuracy: 0.1505 - val_mse: 0.1905\n","Epoch 27/200\n","467/486 [===========================>..] - ETA: 0s - loss: 0.1849 - accuracy: 0.1506 - mse: 0.1904\n","Epoch 27: val_loss improved from 0.19048 to 0.19024, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1848 - accuracy: 0.1505 - mse: 0.1904 - val_loss: 0.1902 - val_accuracy: 0.1505 - val_mse: 0.1902\n","Epoch 28/200\n","472/486 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.1507 - mse: 0.1901\n","Epoch 28: val_loss improved from 0.19024 to 0.19000, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1846 - accuracy: 0.1505 - mse: 0.1901 - val_loss: 0.1900 - val_accuracy: 0.1505 - val_mse: 0.1900\n","Epoch 29/200\n","481/486 [============================>.] - ETA: 0s - loss: 0.1844 - accuracy: 0.1504 - mse: 0.1899\n","Epoch 29: val_loss improved from 0.19000 to 0.18977, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1845 - accuracy: 0.1505 - mse: 0.1899 - val_loss: 0.1898 - val_accuracy: 0.1505 - val_mse: 0.1898\n","Epoch 30/200\n","483/486 [============================>.] - ETA: 0s - loss: 0.1843 - accuracy: 0.1505 - mse: 0.1897\n","Epoch 30: val_loss improved from 0.18977 to 0.18954, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1843 - accuracy: 0.1505 - mse: 0.1897 - val_loss: 0.1895 - val_accuracy: 0.1505 - val_mse: 0.1895\n","Epoch 31/200\n","485/486 [============================>.] - ETA: 0s - loss: 0.1841 - accuracy: 0.1503 - mse: 0.1895\n","Epoch 31: val_loss improved from 0.18954 to 0.18931, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1841 - accuracy: 0.1505 - mse: 0.1894 - val_loss: 0.1893 - val_accuracy: 0.1505 - val_mse: 0.1893\n","Epoch 32/200\n","483/486 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.1505 - mse: 0.1892\n","Epoch 32: val_loss improved from 0.18931 to 0.18908, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1839 - accuracy: 0.1505 - mse: 0.1892 - val_loss: 0.1891 - val_accuracy: 0.1505 - val_mse: 0.1891\n","Epoch 33/200\n","477/486 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.1504 - mse: 0.1889\n","Epoch 33: val_loss improved from 0.18908 to 0.18886, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1838 - accuracy: 0.1505 - mse: 0.1890 - val_loss: 0.1889 - val_accuracy: 0.1505 - val_mse: 0.1889\n","Epoch 34/200\n","476/486 [============================>.] - ETA: 0s - loss: 0.1836 - accuracy: 0.1503 - mse: 0.1887\n","Epoch 34: val_loss improved from 0.18886 to 0.18864, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1836 - accuracy: 0.1505 - mse: 0.1888 - val_loss: 0.1886 - val_accuracy: 0.1505 - val_mse: 0.1886\n","Epoch 35/200\n","485/486 [============================>.] - ETA: 0s - loss: 0.1834 - accuracy: 0.1503 - mse: 0.1886\n","Epoch 35: val_loss improved from 0.18864 to 0.18842, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1834 - accuracy: 0.1505 - mse: 0.1885 - val_loss: 0.1884 - val_accuracy: 0.1505 - val_mse: 0.1884\n","Epoch 36/200\n","471/486 [============================>.] - ETA: 0s - loss: 0.1832 - accuracy: 0.1507 - mse: 0.1883\n","Epoch 36: val_loss improved from 0.18842 to 0.18821, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1833 - accuracy: 0.1505 - mse: 0.1883 - val_loss: 0.1882 - val_accuracy: 0.1505 - val_mse: 0.1882\n","Epoch 37/200\n","478/486 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.1505 - mse: 0.1881\n","Epoch 37: val_loss improved from 0.18821 to 0.18800, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1831 - accuracy: 0.1505 - mse: 0.1881 - val_loss: 0.1880 - val_accuracy: 0.1505 - val_mse: 0.1880\n","Epoch 38/200\n","476/486 [============================>.] - ETA: 0s - loss: 0.1829 - accuracy: 0.1503 - mse: 0.1879\n","Epoch 38: val_loss improved from 0.18800 to 0.18779, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1829 - accuracy: 0.1505 - mse: 0.1879 - val_loss: 0.1878 - val_accuracy: 0.1505 - val_mse: 0.1878\n","Epoch 39/200\n","479/486 [============================>.] - ETA: 0s - loss: 0.1828 - accuracy: 0.1504 - mse: 0.1877\n","Epoch 39: val_loss improved from 0.18779 to 0.18758, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1828 - accuracy: 0.1505 - mse: 0.1877 - val_loss: 0.1876 - val_accuracy: 0.1505 - val_mse: 0.1876\n","Epoch 40/200\n","462/486 [===========================>..] - ETA: 0s - loss: 0.1828 - accuracy: 0.1508 - mse: 0.1874\n","Epoch 40: val_loss improved from 0.18758 to 0.18738, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1826 - accuracy: 0.1505 - mse: 0.1875 - val_loss: 0.1874 - val_accuracy: 0.1505 - val_mse: 0.1874\n","Epoch 41/200\n","481/486 [============================>.] - ETA: 0s - loss: 0.1824 - accuracy: 0.1504 - mse: 0.1873\n","Epoch 41: val_loss improved from 0.18738 to 0.18718, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1825 - accuracy: 0.1505 - mse: 0.1873 - val_loss: 0.1872 - val_accuracy: 0.1505 - val_mse: 0.1872\n","Epoch 42/200\n","485/486 [============================>.] - ETA: 0s - loss: 0.1823 - accuracy: 0.1503 - mse: 0.1871\n","Epoch 42: val_loss improved from 0.18718 to 0.18698, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1823 - accuracy: 0.1505 - mse: 0.1871 - val_loss: 0.1870 - val_accuracy: 0.1505 - val_mse: 0.1870\n","Epoch 43/200\n","471/486 [============================>.] - ETA: 0s - loss: 0.1821 - accuracy: 0.1507 - mse: 0.1868\n","Epoch 43: val_loss improved from 0.18698 to 0.18678, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1822 - accuracy: 0.1505 - mse: 0.1869 - val_loss: 0.1868 - val_accuracy: 0.1505 - val_mse: 0.1868\n","Epoch 44/200\n","473/486 [============================>.] - ETA: 0s - loss: 0.1820 - accuracy: 0.1506 - mse: 0.1866\n","Epoch 44: val_loss improved from 0.18678 to 0.18659, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1820 - accuracy: 0.1505 - mse: 0.1867 - val_loss: 0.1866 - val_accuracy: 0.1505 - val_mse: 0.1866\n","Epoch 45/200\n","486/486 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.1505 - mse: 0.1865\n","Epoch 45: val_loss improved from 0.18659 to 0.18639, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1819 - accuracy: 0.1505 - mse: 0.1865 - val_loss: 0.1864 - val_accuracy: 0.1505 - val_mse: 0.1864\n","Epoch 46/200\n","463/486 [===========================>..] - ETA: 0s - loss: 0.1820 - accuracy: 0.1506 - mse: 0.1863\n","Epoch 46: val_loss improved from 0.18639 to 0.18620, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.1505 - mse: 0.1863 - val_loss: 0.1862 - val_accuracy: 0.1505 - val_mse: 0.1862\n","Epoch 47/200\n","470/486 [============================>.] - ETA: 0s - loss: 0.1816 - accuracy: 0.1506 - mse: 0.1861\n","Epoch 47: val_loss improved from 0.18620 to 0.18601, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1816 - accuracy: 0.1505 - mse: 0.1861 - val_loss: 0.1860 - val_accuracy: 0.1505 - val_mse: 0.1860\n","Epoch 48/200\n","479/486 [============================>.] - ETA: 0s - loss: 0.1815 - accuracy: 0.1504 - mse: 0.1859\n","Epoch 48: val_loss improved from 0.18601 to 0.18583, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1815 - accuracy: 0.1505 - mse: 0.1859 - val_loss: 0.1858 - val_accuracy: 0.1505 - val_mse: 0.1858\n","Epoch 49/200\n","479/486 [============================>.] - ETA: 0s - loss: 0.1813 - accuracy: 0.1504 - mse: 0.1857\n","Epoch 49: val_loss improved from 0.18583 to 0.18564, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.1505 - mse: 0.1858 - val_loss: 0.1856 - val_accuracy: 0.1505 - val_mse: 0.1856\n","Epoch 50/200\n","463/486 [===========================>..] - ETA: 0s - loss: 0.1814 - accuracy: 0.1506 - mse: 0.1855\n","Epoch 50: val_loss improved from 0.18564 to 0.18546, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1812 - accuracy: 0.1505 - mse: 0.1856 - val_loss: 0.1855 - val_accuracy: 0.1505 - val_mse: 0.1855\n","Epoch 51/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1810 - accuracy: 0.1503 - mse: 0.1854\n","Epoch 51: val_loss improved from 0.18546 to 0.18528, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1810 - accuracy: 0.1505 - mse: 0.1854 - val_loss: 0.1853 - val_accuracy: 0.1505 - val_mse: 0.1853\n","Epoch 52/200\n","472/486 [============================>.] - ETA: 0s - loss: 0.1808 - accuracy: 0.1507 - mse: 0.1851\n","Epoch 52: val_loss improved from 0.18528 to 0.18510, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1809 - accuracy: 0.1505 - mse: 0.1852 - val_loss: 0.1851 - val_accuracy: 0.1505 - val_mse: 0.1851\n","Epoch 53/200\n","476/486 [============================>.] - ETA: 0s - loss: 0.1808 - accuracy: 0.1503 - mse: 0.1850\n","Epoch 53: val_loss improved from 0.18510 to 0.18492, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1808 - accuracy: 0.1505 - mse: 0.1850 - val_loss: 0.1849 - val_accuracy: 0.1505 - val_mse: 0.1849\n","Epoch 54/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1806 - accuracy: 0.1503 - mse: 0.1849\n","Epoch 54: val_loss improved from 0.18492 to 0.18475, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1806 - accuracy: 0.1505 - mse: 0.1848 - val_loss: 0.1847 - val_accuracy: 0.1505 - val_mse: 0.1847\n","Epoch 55/200\n","475/486 [============================>.] - ETA: 0s - loss: 0.1805 - accuracy: 0.1504 - mse: 0.1846\n","Epoch 55: val_loss improved from 0.18475 to 0.18457, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1805 - accuracy: 0.1505 - mse: 0.1847 - val_loss: 0.1846 - val_accuracy: 0.1505 - val_mse: 0.1846\n","Epoch 56/200\n","472/486 [============================>.] - ETA: 0s - loss: 0.1803 - accuracy: 0.1507 - mse: 0.1844\n","Epoch 56: val_loss improved from 0.18457 to 0.18440, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1804 - accuracy: 0.1505 - mse: 0.1845 - val_loss: 0.1844 - val_accuracy: 0.1505 - val_mse: 0.1844\n","Epoch 57/200\n","463/486 [===========================>..] - ETA: 0s - loss: 0.1805 - accuracy: 0.1506 - mse: 0.1843\n","Epoch 57: val_loss improved from 0.18440 to 0.18423, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1803 - accuracy: 0.1505 - mse: 0.1843 - val_loss: 0.1842 - val_accuracy: 0.1505 - val_mse: 0.1842\n","Epoch 58/200\n","480/486 [============================>.] - ETA: 0s - loss: 0.1801 - accuracy: 0.1505 - mse: 0.1841\n","Epoch 58: val_loss improved from 0.18423 to 0.18406, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1801 - accuracy: 0.1505 - mse: 0.1842 - val_loss: 0.1841 - val_accuracy: 0.1505 - val_mse: 0.1841\n","Epoch 59/200\n","466/486 [===========================>..] - ETA: 0s - loss: 0.1801 - accuracy: 0.1507 - mse: 0.1840\n","Epoch 59: val_loss improved from 0.18406 to 0.18389, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1800 - accuracy: 0.1505 - mse: 0.1840 - val_loss: 0.1839 - val_accuracy: 0.1505 - val_mse: 0.1839\n","Epoch 60/200\n","463/486 [===========================>..] - ETA: 0s - loss: 0.1801 - accuracy: 0.1506 - mse: 0.1838\n","Epoch 60: val_loss improved from 0.18389 to 0.18372, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1799 - accuracy: 0.1505 - mse: 0.1838 - val_loss: 0.1837 - val_accuracy: 0.1505 - val_mse: 0.1837\n","Epoch 61/200\n","476/486 [============================>.] - ETA: 0s - loss: 0.1798 - accuracy: 0.1503 - mse: 0.1836\n","Epoch 61: val_loss improved from 0.18372 to 0.18356, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1798 - accuracy: 0.1505 - mse: 0.1837 - val_loss: 0.1836 - val_accuracy: 0.1505 - val_mse: 0.1836\n","Epoch 62/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1796 - accuracy: 0.1503 - mse: 0.1835\n","Epoch 62: val_loss improved from 0.18356 to 0.18339, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1797 - accuracy: 0.1505 - mse: 0.1835 - val_loss: 0.1834 - val_accuracy: 0.1505 - val_mse: 0.1834\n","Epoch 63/200\n","479/486 [============================>.] - ETA: 0s - loss: 0.1795 - accuracy: 0.1504 - mse: 0.1833\n","Epoch 63: val_loss improved from 0.18339 to 0.18323, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1795 - accuracy: 0.1505 - mse: 0.1833 - val_loss: 0.1832 - val_accuracy: 0.1505 - val_mse: 0.1832\n","Epoch 64/200\n","486/486 [==============================] - ETA: 0s - loss: 0.1794 - accuracy: 0.1505 - mse: 0.1832\n","Epoch 64: val_loss improved from 0.18323 to 0.18307, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1794 - accuracy: 0.1505 - mse: 0.1832 - val_loss: 0.1831 - val_accuracy: 0.1505 - val_mse: 0.1831\n","Epoch 65/200\n","463/486 [===========================>..] - ETA: 0s - loss: 0.1795 - accuracy: 0.1506 - mse: 0.1830\n","Epoch 65: val_loss improved from 0.18307 to 0.18291, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1793 - accuracy: 0.1505 - mse: 0.1830 - val_loss: 0.1829 - val_accuracy: 0.1505 - val_mse: 0.1829\n","Epoch 66/200\n","469/486 [===========================>..] - ETA: 0s - loss: 0.1792 - accuracy: 0.1508 - mse: 0.1828\n","Epoch 66: val_loss improved from 0.18291 to 0.18275, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1792 - accuracy: 0.1505 - mse: 0.1828 - val_loss: 0.1828 - val_accuracy: 0.1505 - val_mse: 0.1828\n","Epoch 67/200\n","466/486 [===========================>..] - ETA: 0s - loss: 0.1792 - accuracy: 0.1507 - mse: 0.1827\n","Epoch 67: val_loss improved from 0.18275 to 0.18259, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1791 - accuracy: 0.1505 - mse: 0.1827 - val_loss: 0.1826 - val_accuracy: 0.1505 - val_mse: 0.1826\n","Epoch 68/200\n","481/486 [============================>.] - ETA: 0s - loss: 0.1789 - accuracy: 0.1504 - mse: 0.1825\n","Epoch 68: val_loss improved from 0.18259 to 0.18244, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1790 - accuracy: 0.1505 - mse: 0.1825 - val_loss: 0.1824 - val_accuracy: 0.1505 - val_mse: 0.1824\n","Epoch 69/200\n","470/486 [============================>.] - ETA: 0s - loss: 0.1789 - accuracy: 0.1506 - mse: 0.1823\n","Epoch 69: val_loss improved from 0.18244 to 0.18228, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1788 - accuracy: 0.1505 - mse: 0.1824 - val_loss: 0.1823 - val_accuracy: 0.1505 - val_mse: 0.1823\n","Epoch 70/200\n","481/486 [============================>.] - ETA: 0s - loss: 0.1786 - accuracy: 0.1504 - mse: 0.1822\n","Epoch 70: val_loss improved from 0.18228 to 0.18213, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1787 - accuracy: 0.1505 - mse: 0.1822 - val_loss: 0.1821 - val_accuracy: 0.1505 - val_mse: 0.1821\n","Epoch 71/200\n","469/486 [===========================>..] - ETA: 0s - loss: 0.1787 - accuracy: 0.1508 - mse: 0.1820\n","Epoch 71: val_loss improved from 0.18213 to 0.18197, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1786 - accuracy: 0.1505 - mse: 0.1821 - val_loss: 0.1820 - val_accuracy: 0.1505 - val_mse: 0.1820\n","Epoch 72/200\n","485/486 [============================>.] - ETA: 0s - loss: 0.1785 - accuracy: 0.1503 - mse: 0.1819\n","Epoch 72: val_loss improved from 0.18197 to 0.18182, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1785 - accuracy: 0.1505 - mse: 0.1819 - val_loss: 0.1818 - val_accuracy: 0.1505 - val_mse: 0.1818\n","Epoch 73/200\n","483/486 [============================>.] - ETA: 0s - loss: 0.1784 - accuracy: 0.1505 - mse: 0.1817\n","Epoch 73: val_loss improved from 0.18182 to 0.18167, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1784 - accuracy: 0.1505 - mse: 0.1818 - val_loss: 0.1817 - val_accuracy: 0.1505 - val_mse: 0.1817\n","Epoch 74/200\n","464/486 [===========================>..] - ETA: 0s - loss: 0.1785 - accuracy: 0.1509 - mse: 0.1816\n","Epoch 74: val_loss improved from 0.18167 to 0.18152, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1783 - accuracy: 0.1505 - mse: 0.1816 - val_loss: 0.1815 - val_accuracy: 0.1505 - val_mse: 0.1815\n","Epoch 75/200\n","464/486 [===========================>..] - ETA: 0s - loss: 0.1784 - accuracy: 0.1509 - mse: 0.1814\n","Epoch 75: val_loss improved from 0.18152 to 0.18137, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1782 - accuracy: 0.1505 - mse: 0.1815 - val_loss: 0.1814 - val_accuracy: 0.1505 - val_mse: 0.1814\n","Epoch 76/200\n","469/486 [===========================>..] - ETA: 0s - loss: 0.1781 - accuracy: 0.1508 - mse: 0.1812\n","Epoch 76: val_loss improved from 0.18137 to 0.18123, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1781 - accuracy: 0.1505 - mse: 0.1813 - val_loss: 0.1812 - val_accuracy: 0.1505 - val_mse: 0.1812\n","Epoch 77/200\n","479/486 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.1504 - mse: 0.1811\n","Epoch 77: val_loss improved from 0.18123 to 0.18108, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1780 - accuracy: 0.1505 - mse: 0.1812 - val_loss: 0.1811 - val_accuracy: 0.1505 - val_mse: 0.1811\n","Epoch 78/200\n","472/486 [============================>.] - ETA: 0s - loss: 0.1778 - accuracy: 0.1507 - mse: 0.1809\n","Epoch 78: val_loss improved from 0.18108 to 0.18093, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1779 - accuracy: 0.1505 - mse: 0.1810 - val_loss: 0.1809 - val_accuracy: 0.1505 - val_mse: 0.1809\n","Epoch 79/200\n","474/486 [============================>.] - ETA: 0s - loss: 0.1777 - accuracy: 0.1505 - mse: 0.1808\n","Epoch 79: val_loss improved from 0.18093 to 0.18079, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1778 - accuracy: 0.1505 - mse: 0.1809 - val_loss: 0.1808 - val_accuracy: 0.1505 - val_mse: 0.1808\n","Epoch 80/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1776 - accuracy: 0.1503 - mse: 0.1807\n","Epoch 80: val_loss improved from 0.18079 to 0.18064, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1777 - accuracy: 0.1505 - mse: 0.1807 - val_loss: 0.1806 - val_accuracy: 0.1505 - val_mse: 0.1806\n","Epoch 81/200\n","480/486 [============================>.] - ETA: 0s - loss: 0.1775 - accuracy: 0.1505 - mse: 0.1806\n","Epoch 81: val_loss improved from 0.18064 to 0.18050, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1776 - accuracy: 0.1505 - mse: 0.1806 - val_loss: 0.1805 - val_accuracy: 0.1505 - val_mse: 0.1805\n","Epoch 82/200\n","484/486 [============================>.] - ETA: 0s - loss: 0.1775 - accuracy: 0.1503 - mse: 0.1805\n","Epoch 82: val_loss improved from 0.18050 to 0.18036, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1775 - accuracy: 0.1505 - mse: 0.1804 - val_loss: 0.1804 - val_accuracy: 0.1505 - val_mse: 0.1804\n","Epoch 83/200\n","463/486 [===========================>..] - ETA: 0s - loss: 0.1776 - accuracy: 0.1506 - mse: 0.1803\n","Epoch 83: val_loss improved from 0.18036 to 0.18022, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1774 - accuracy: 0.1505 - mse: 0.1803 - val_loss: 0.1802 - val_accuracy: 0.1505 - val_mse: 0.1802\n","Epoch 84/200\n","468/486 [===========================>..] - ETA: 0s - loss: 0.1774 - accuracy: 0.1506 - mse: 0.1801\n","Epoch 84: val_loss improved from 0.18022 to 0.18008, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1773 - accuracy: 0.1505 - mse: 0.1802 - val_loss: 0.1801 - val_accuracy: 0.1505 - val_mse: 0.1801\n","Epoch 85/200\n","474/486 [============================>.] - ETA: 0s - loss: 0.1771 - accuracy: 0.1505 - mse: 0.1800\n","Epoch 85: val_loss improved from 0.18008 to 0.17994, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1772 - accuracy: 0.1505 - mse: 0.1800 - val_loss: 0.1799 - val_accuracy: 0.1505 - val_mse: 0.1799\n","Epoch 86/200\n","465/486 [===========================>..] - ETA: 0s - loss: 0.1772 - accuracy: 0.1507 - mse: 0.1798\n","Epoch 86: val_loss improved from 0.17994 to 0.17980, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1771 - accuracy: 0.1505 - mse: 0.1799 - val_loss: 0.1798 - val_accuracy: 0.1505 - val_mse: 0.1798\n","Epoch 87/200\n","467/486 [===========================>..] - ETA: 0s - loss: 0.1771 - accuracy: 0.1506 - mse: 0.1797\n","Epoch 87: val_loss improved from 0.17980 to 0.17966, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1770 - accuracy: 0.1505 - mse: 0.1797 - val_loss: 0.1797 - val_accuracy: 0.1505 - val_mse: 0.1797\n","Epoch 88/200\n","462/486 [===========================>..] - ETA: 0s - loss: 0.1771 - accuracy: 0.1508 - mse: 0.1796\n","Epoch 88: val_loss improved from 0.17966 to 0.17952, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1769 - accuracy: 0.1505 - mse: 0.1796 - val_loss: 0.1795 - val_accuracy: 0.1505 - val_mse: 0.1795\n","Epoch 89/200\n","464/486 [===========================>..] - ETA: 0s - loss: 0.1770 - accuracy: 0.1509 - mse: 0.1794\n","Epoch 89: val_loss improved from 0.17952 to 0.17938, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.1505 - mse: 0.1795 - val_loss: 0.1794 - val_accuracy: 0.1505 - val_mse: 0.1794\n","Epoch 90/200\n","470/486 [============================>.] - ETA: 0s - loss: 0.1767 - accuracy: 0.1506 - mse: 0.1793\n","Epoch 90: val_loss improved from 0.17938 to 0.17925, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1767 - accuracy: 0.1505 - mse: 0.1793 - val_loss: 0.1792 - val_accuracy: 0.1505 - val_mse: 0.1792\n","Epoch 91/200\n","470/486 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.1506 - mse: 0.1791\n","Epoch 91: val_loss improved from 0.17925 to 0.17911, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1766 - accuracy: 0.1505 - mse: 0.1792 - val_loss: 0.1791 - val_accuracy: 0.1505 - val_mse: 0.1791\n","Epoch 92/200\n","472/486 [============================>.] - ETA: 0s - loss: 0.1764 - accuracy: 0.1507 - mse: 0.1790\n","Epoch 92: val_loss improved from 0.17911 to 0.17898, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1765 - accuracy: 0.1505 - mse: 0.1791 - val_loss: 0.1790 - val_accuracy: 0.1505 - val_mse: 0.1790\n","Epoch 93/200\n","468/486 [===========================>..] - ETA: 0s - loss: 0.1765 - accuracy: 0.1506 - mse: 0.1789\n","Epoch 93: val_loss improved from 0.17898 to 0.17885, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.1505 - mse: 0.1789 - val_loss: 0.1788 - val_accuracy: 0.1505 - val_mse: 0.1788\n","Epoch 94/200\n","467/486 [===========================>..] - ETA: 0s - loss: 0.1764 - accuracy: 0.1506 - mse: 0.1788\n","Epoch 94: val_loss improved from 0.17885 to 0.17871, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1763 - accuracy: 0.1505 - mse: 0.1788 - val_loss: 0.1787 - val_accuracy: 0.1505 - val_mse: 0.1787\n","Epoch 95/200\n","466/486 [===========================>..] - ETA: 0s - loss: 0.1763 - accuracy: 0.1507 - mse: 0.1786\n","Epoch 95: val_loss improved from 0.17871 to 0.17858, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.1505 - mse: 0.1787 - val_loss: 0.1786 - val_accuracy: 0.1505 - val_mse: 0.1786\n","Epoch 96/200\n","465/486 [===========================>..] - ETA: 0s - loss: 0.1762 - accuracy: 0.1507 - mse: 0.1785\n","Epoch 96: val_loss improved from 0.17858 to 0.17845, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1761 - accuracy: 0.1505 - mse: 0.1785 - val_loss: 0.1784 - val_accuracy: 0.1505 - val_mse: 0.1784\n","Epoch 97/200\n","480/486 [============================>.] - ETA: 0s - loss: 0.1760 - accuracy: 0.1505 - mse: 0.1784\n","Epoch 97: val_loss improved from 0.17845 to 0.17832, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1760 - accuracy: 0.1505 - mse: 0.1784 - val_loss: 0.1783 - val_accuracy: 0.1505 - val_mse: 0.1783\n","Epoch 98/200\n","471/486 [============================>.] - ETA: 0s - loss: 0.1759 - accuracy: 0.1507 - mse: 0.1782\n","Epoch 98: val_loss improved from 0.17832 to 0.17819, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1759 - accuracy: 0.1505 - mse: 0.1783 - val_loss: 0.1782 - val_accuracy: 0.1505 - val_mse: 0.1782\n","Epoch 99/200\n","467/486 [===========================>..] - ETA: 0s - loss: 0.1759 - accuracy: 0.1506 - mse: 0.1781\n","Epoch 99: val_loss improved from 0.17819 to 0.17806, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 2ms/step - loss: 0.1758 - accuracy: 0.1505 - mse: 0.1781 - val_loss: 0.1781 - val_accuracy: 0.1505 - val_mse: 0.1781\n","Epoch 100/200\n","484/486 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.1503 - mse: 0.1780\n","Epoch 100: val_loss improved from 0.17806 to 0.17793, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1757 - accuracy: 0.1505 - mse: 0.1780 - val_loss: 0.1779 - val_accuracy: 0.1505 - val_mse: 0.1779\n","Epoch 101/200\n","479/486 [============================>.] - ETA: 0s - loss: 0.1756 - accuracy: 0.1504 - mse: 0.1778\n","Epoch 101: val_loss improved from 0.17793 to 0.17780, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1757 - accuracy: 0.1505 - mse: 0.1779 - val_loss: 0.1778 - val_accuracy: 0.1505 - val_mse: 0.1778\n","Epoch 102/200\n","461/486 [===========================>..] - ETA: 0s - loss: 0.1758 - accuracy: 0.1508 - mse: 0.1777\n","Epoch 102: val_loss improved from 0.17780 to 0.17768, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1756 - accuracy: 0.1505 - mse: 0.1778 - val_loss: 0.1777 - val_accuracy: 0.1505 - val_mse: 0.1777\n","Epoch 103/200\n","483/486 [============================>.] - ETA: 0s - loss: 0.1755 - accuracy: 0.1505 - mse: 0.1776\n","Epoch 103: val_loss improved from 0.17768 to 0.17755, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1755 - accuracy: 0.1505 - mse: 0.1776 - val_loss: 0.1775 - val_accuracy: 0.1505 - val_mse: 0.1775\n","Epoch 104/200\n","465/486 [===========================>..] - ETA: 0s - loss: 0.1755 - accuracy: 0.1507 - mse: 0.1775\n","Epoch 104: val_loss improved from 0.17755 to 0.17742, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1754 - accuracy: 0.1505 - mse: 0.1775 - val_loss: 0.1774 - val_accuracy: 0.1505 - val_mse: 0.1774\n","Epoch 105/200\n","463/486 [===========================>..] - ETA: 0s - loss: 0.1755 - accuracy: 0.1506 - mse: 0.1773\n","Epoch 105: val_loss improved from 0.17742 to 0.17730, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1753 - accuracy: 0.1505 - mse: 0.1774 - val_loss: 0.1773 - val_accuracy: 0.1505 - val_mse: 0.1773\n","Epoch 106/200\n","466/486 [===========================>..] - ETA: 0s - loss: 0.1753 - accuracy: 0.1507 - mse: 0.1772\n","Epoch 106: val_loss improved from 0.17730 to 0.17717, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1752 - accuracy: 0.1505 - mse: 0.1773 - val_loss: 0.1772 - val_accuracy: 0.1505 - val_mse: 0.1772\n","Epoch 107/200\n","471/486 [============================>.] - ETA: 0s - loss: 0.1751 - accuracy: 0.1507 - mse: 0.1771\n","Epoch 107: val_loss improved from 0.17717 to 0.17705, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1751 - accuracy: 0.1505 - mse: 0.1771 - val_loss: 0.1770 - val_accuracy: 0.1505 - val_mse: 0.1770\n","Epoch 108/200\n","468/486 [===========================>..] - ETA: 0s - loss: 0.1751 - accuracy: 0.1506 - mse: 0.1770\n","Epoch 108: val_loss improved from 0.17705 to 0.17692, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1750 - accuracy: 0.1505 - mse: 0.1770 - val_loss: 0.1769 - val_accuracy: 0.1505 - val_mse: 0.1769\n","Epoch 109/200\n","481/486 [============================>.] - ETA: 0s - loss: 0.1749 - accuracy: 0.1504 - mse: 0.1769\n","Epoch 109: val_loss improved from 0.17692 to 0.17680, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1750 - accuracy: 0.1505 - mse: 0.1769 - val_loss: 0.1768 - val_accuracy: 0.1505 - val_mse: 0.1768\n","Epoch 110/200\n","486/486 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.1505 - mse: 0.1768\n","Epoch 110: val_loss improved from 0.17680 to 0.17668, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1749 - accuracy: 0.1505 - mse: 0.1768 - val_loss: 0.1767 - val_accuracy: 0.1505 - val_mse: 0.1767\n","Epoch 111/200\n","462/486 [===========================>..] - ETA: 0s - loss: 0.1750 - accuracy: 0.1508 - mse: 0.1766\n","Epoch 111: val_loss improved from 0.17668 to 0.17656, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1748 - accuracy: 0.1505 - mse: 0.1766 - val_loss: 0.1766 - val_accuracy: 0.1505 - val_mse: 0.1766\n","Epoch 112/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1746 - accuracy: 0.1503 - mse: 0.1765\n","Epoch 112: val_loss improved from 0.17656 to 0.17644, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1747 - accuracy: 0.1505 - mse: 0.1765 - val_loss: 0.1764 - val_accuracy: 0.1505 - val_mse: 0.1764\n","Epoch 113/200\n","461/486 [===========================>..] - ETA: 0s - loss: 0.1748 - accuracy: 0.1508 - mse: 0.1763\n","Epoch 113: val_loss improved from 0.17644 to 0.17632, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1746 - accuracy: 0.1505 - mse: 0.1764 - val_loss: 0.1763 - val_accuracy: 0.1505 - val_mse: 0.1763\n","Epoch 114/200\n","475/486 [============================>.] - ETA: 0s - loss: 0.1745 - accuracy: 0.1504 - mse: 0.1762\n","Epoch 114: val_loss improved from 0.17632 to 0.17619, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1745 - accuracy: 0.1505 - mse: 0.1763 - val_loss: 0.1762 - val_accuracy: 0.1505 - val_mse: 0.1762\n","Epoch 115/200\n","486/486 [==============================] - ETA: 0s - loss: 0.1745 - accuracy: 0.1505 - mse: 0.1762\n","Epoch 115: val_loss improved from 0.17619 to 0.17608, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1745 - accuracy: 0.1505 - mse: 0.1762 - val_loss: 0.1761 - val_accuracy: 0.1505 - val_mse: 0.1761\n","Epoch 116/200\n","475/486 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.1504 - mse: 0.1760\n","Epoch 116: val_loss improved from 0.17608 to 0.17596, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1744 - accuracy: 0.1505 - mse: 0.1760 - val_loss: 0.1760 - val_accuracy: 0.1505 - val_mse: 0.1760\n","Epoch 117/200\n","480/486 [============================>.] - ETA: 0s - loss: 0.1742 - accuracy: 0.1505 - mse: 0.1759\n","Epoch 117: val_loss improved from 0.17596 to 0.17584, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1743 - accuracy: 0.1505 - mse: 0.1759 - val_loss: 0.1758 - val_accuracy: 0.1505 - val_mse: 0.1758\n","Epoch 118/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1741 - accuracy: 0.1503 - mse: 0.1758\n","Epoch 118: val_loss improved from 0.17584 to 0.17572, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1742 - accuracy: 0.1505 - mse: 0.1758 - val_loss: 0.1757 - val_accuracy: 0.1505 - val_mse: 0.1757\n","Epoch 119/200\n","484/486 [============================>.] - ETA: 0s - loss: 0.1741 - accuracy: 0.1503 - mse: 0.1757\n","Epoch 119: val_loss improved from 0.17572 to 0.17560, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1741 - accuracy: 0.1505 - mse: 0.1757 - val_loss: 0.1756 - val_accuracy: 0.1505 - val_mse: 0.1756\n","Epoch 120/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1740 - accuracy: 0.1503 - mse: 0.1756\n","Epoch 120: val_loss improved from 0.17560 to 0.17549, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1740 - accuracy: 0.1505 - mse: 0.1756 - val_loss: 0.1755 - val_accuracy: 0.1505 - val_mse: 0.1755\n","Epoch 121/200\n","469/486 [===========================>..] - ETA: 0s - loss: 0.1740 - accuracy: 0.1508 - mse: 0.1754\n","Epoch 121: val_loss improved from 0.17549 to 0.17537, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1740 - accuracy: 0.1505 - mse: 0.1754 - val_loss: 0.1754 - val_accuracy: 0.1505 - val_mse: 0.1754\n","Epoch 122/200\n","465/486 [===========================>..] - ETA: 0s - loss: 0.1740 - accuracy: 0.1507 - mse: 0.1753\n","Epoch 122: val_loss improved from 0.17537 to 0.17525, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1739 - accuracy: 0.1505 - mse: 0.1753 - val_loss: 0.1753 - val_accuracy: 0.1505 - val_mse: 0.1753\n","Epoch 123/200\n","477/486 [============================>.] - ETA: 0s - loss: 0.1738 - accuracy: 0.1504 - mse: 0.1752\n","Epoch 123: val_loss improved from 0.17525 to 0.17514, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1738 - accuracy: 0.1505 - mse: 0.1752 - val_loss: 0.1751 - val_accuracy: 0.1505 - val_mse: 0.1751\n","Epoch 124/200\n","483/486 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.1505 - mse: 0.1751\n","Epoch 124: val_loss improved from 0.17514 to 0.17503, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1737 - accuracy: 0.1505 - mse: 0.1751 - val_loss: 0.1750 - val_accuracy: 0.1505 - val_mse: 0.1750\n","Epoch 125/200\n","472/486 [============================>.] - ETA: 0s - loss: 0.1736 - accuracy: 0.1507 - mse: 0.1749\n","Epoch 125: val_loss improved from 0.17503 to 0.17491, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1736 - accuracy: 0.1505 - mse: 0.1750 - val_loss: 0.1749 - val_accuracy: 0.1505 - val_mse: 0.1749\n","Epoch 126/200\n","477/486 [============================>.] - ETA: 0s - loss: 0.1736 - accuracy: 0.1504 - mse: 0.1748\n","Epoch 126: val_loss improved from 0.17491 to 0.17480, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1736 - accuracy: 0.1505 - mse: 0.1749 - val_loss: 0.1748 - val_accuracy: 0.1505 - val_mse: 0.1748\n","Epoch 127/200\n","471/486 [============================>.] - ETA: 0s - loss: 0.1735 - accuracy: 0.1507 - mse: 0.1747\n","Epoch 127: val_loss improved from 0.17480 to 0.17469, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1735 - accuracy: 0.1505 - mse: 0.1748 - val_loss: 0.1747 - val_accuracy: 0.1505 - val_mse: 0.1747\n","Epoch 128/200\n","480/486 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.1505 - mse: 0.1746\n","Epoch 128: val_loss improved from 0.17469 to 0.17457, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1734 - accuracy: 0.1505 - mse: 0.1746 - val_loss: 0.1746 - val_accuracy: 0.1505 - val_mse: 0.1746\n","Epoch 129/200\n","477/486 [============================>.] - ETA: 0s - loss: 0.1733 - accuracy: 0.1504 - mse: 0.1745\n","Epoch 129: val_loss improved from 0.17457 to 0.17446, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1733 - accuracy: 0.1505 - mse: 0.1745 - val_loss: 0.1745 - val_accuracy: 0.1505 - val_mse: 0.1745\n","Epoch 130/200\n","484/486 [============================>.] - ETA: 0s - loss: 0.1733 - accuracy: 0.1503 - mse: 0.1744\n","Epoch 130: val_loss improved from 0.17446 to 0.17435, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1733 - accuracy: 0.1505 - mse: 0.1744 - val_loss: 0.1743 - val_accuracy: 0.1505 - val_mse: 0.1743\n","Epoch 131/200\n","471/486 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.1507 - mse: 0.1742\n","Epoch 131: val_loss improved from 0.17435 to 0.17424, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1732 - accuracy: 0.1505 - mse: 0.1743 - val_loss: 0.1742 - val_accuracy: 0.1505 - val_mse: 0.1742\n","Epoch 132/200\n","485/486 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.1503 - mse: 0.1742\n","Epoch 132: val_loss improved from 0.17424 to 0.17413, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1731 - accuracy: 0.1505 - mse: 0.1742 - val_loss: 0.1741 - val_accuracy: 0.1505 - val_mse: 0.1741\n","Epoch 133/200\n","470/486 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.1506 - mse: 0.1740\n","Epoch 133: val_loss improved from 0.17413 to 0.17402, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1730 - accuracy: 0.1505 - mse: 0.1741 - val_loss: 0.1740 - val_accuracy: 0.1505 - val_mse: 0.1740\n","Epoch 134/200\n","467/486 [===========================>..] - ETA: 0s - loss: 0.1731 - accuracy: 0.1506 - mse: 0.1740\n","Epoch 134: val_loss improved from 0.17402 to 0.17391, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1730 - accuracy: 0.1505 - mse: 0.1740 - val_loss: 0.1739 - val_accuracy: 0.1505 - val_mse: 0.1739\n","Epoch 135/200\n","478/486 [============================>.] - ETA: 0s - loss: 0.1728 - accuracy: 0.1505 - mse: 0.1738\n","Epoch 135: val_loss improved from 0.17391 to 0.17380, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1729 - accuracy: 0.1505 - mse: 0.1739 - val_loss: 0.1738 - val_accuracy: 0.1505 - val_mse: 0.1738\n","Epoch 136/200\n","463/486 [===========================>..] - ETA: 0s - loss: 0.1730 - accuracy: 0.1506 - mse: 0.1737\n","Epoch 136: val_loss improved from 0.17380 to 0.17369, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1728 - accuracy: 0.1505 - mse: 0.1738 - val_loss: 0.1737 - val_accuracy: 0.1505 - val_mse: 0.1737\n","Epoch 137/200\n","472/486 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.1507 - mse: 0.1736\n","Epoch 137: val_loss improved from 0.17369 to 0.17359, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1727 - accuracy: 0.1505 - mse: 0.1737 - val_loss: 0.1736 - val_accuracy: 0.1505 - val_mse: 0.1736\n","Epoch 138/200\n","486/486 [==============================] - ETA: 0s - loss: 0.1727 - accuracy: 0.1505 - mse: 0.1735\n","Epoch 138: val_loss improved from 0.17359 to 0.17348, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1727 - accuracy: 0.1505 - mse: 0.1735 - val_loss: 0.1735 - val_accuracy: 0.1505 - val_mse: 0.1735\n","Epoch 139/200\n","479/486 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.1504 - mse: 0.1734\n","Epoch 139: val_loss improved from 0.17348 to 0.17337, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1726 - accuracy: 0.1505 - mse: 0.1734 - val_loss: 0.1734 - val_accuracy: 0.1505 - val_mse: 0.1734\n","Epoch 140/200\n","480/486 [============================>.] - ETA: 0s - loss: 0.1725 - accuracy: 0.1505 - mse: 0.1733\n","Epoch 140: val_loss improved from 0.17337 to 0.17327, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1725 - accuracy: 0.1505 - mse: 0.1733 - val_loss: 0.1733 - val_accuracy: 0.1505 - val_mse: 0.1733\n","Epoch 141/200\n","469/486 [===========================>..] - ETA: 0s - loss: 0.1725 - accuracy: 0.1508 - mse: 0.1732\n","Epoch 141: val_loss improved from 0.17327 to 0.17316, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1724 - accuracy: 0.1505 - mse: 0.1732 - val_loss: 0.1732 - val_accuracy: 0.1505 - val_mse: 0.1732\n","Epoch 142/200\n","469/486 [===========================>..] - ETA: 0s - loss: 0.1724 - accuracy: 0.1508 - mse: 0.1731\n","Epoch 142: val_loss improved from 0.17316 to 0.17305, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1724 - accuracy: 0.1505 - mse: 0.1731 - val_loss: 0.1731 - val_accuracy: 0.1505 - val_mse: 0.1731\n","Epoch 143/200\n","468/486 [===========================>..] - ETA: 0s - loss: 0.1724 - accuracy: 0.1506 - mse: 0.1730\n","Epoch 143: val_loss improved from 0.17305 to 0.17295, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1723 - accuracy: 0.1505 - mse: 0.1730 - val_loss: 0.1729 - val_accuracy: 0.1505 - val_mse: 0.1729\n","Epoch 144/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1721 - accuracy: 0.1503 - mse: 0.1729\n","Epoch 144: val_loss improved from 0.17295 to 0.17285, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1722 - accuracy: 0.1505 - mse: 0.1729 - val_loss: 0.1728 - val_accuracy: 0.1505 - val_mse: 0.1728\n","Epoch 145/200\n","470/486 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.1506 - mse: 0.1727\n","Epoch 145: val_loss improved from 0.17285 to 0.17274, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1722 - accuracy: 0.1505 - mse: 0.1728 - val_loss: 0.1727 - val_accuracy: 0.1505 - val_mse: 0.1727\n","Epoch 146/200\n","477/486 [============================>.] - ETA: 0s - loss: 0.1721 - accuracy: 0.1504 - mse: 0.1727\n","Epoch 146: val_loss improved from 0.17274 to 0.17264, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1721 - accuracy: 0.1505 - mse: 0.1727 - val_loss: 0.1726 - val_accuracy: 0.1505 - val_mse: 0.1726\n","Epoch 147/200\n","478/486 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.1505 - mse: 0.1726\n","Epoch 147: val_loss improved from 0.17264 to 0.17254, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1720 - accuracy: 0.1505 - mse: 0.1726 - val_loss: 0.1725 - val_accuracy: 0.1505 - val_mse: 0.1725\n","Epoch 148/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.1503 - mse: 0.1725\n","Epoch 148: val_loss improved from 0.17254 to 0.17243, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1720 - accuracy: 0.1505 - mse: 0.1725 - val_loss: 0.1724 - val_accuracy: 0.1505 - val_mse: 0.1724\n","Epoch 149/200\n","476/486 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.1503 - mse: 0.1724\n","Epoch 149: val_loss improved from 0.17243 to 0.17233, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1719 - accuracy: 0.1505 - mse: 0.1724 - val_loss: 0.1723 - val_accuracy: 0.1505 - val_mse: 0.1723\n","Epoch 150/200\n","478/486 [============================>.] - ETA: 0s - loss: 0.1717 - accuracy: 0.1505 - mse: 0.1722\n","Epoch 150: val_loss improved from 0.17233 to 0.17223, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1718 - accuracy: 0.1505 - mse: 0.1723 - val_loss: 0.1722 - val_accuracy: 0.1505 - val_mse: 0.1722\n","Epoch 151/200\n","471/486 [============================>.] - ETA: 0s - loss: 0.1717 - accuracy: 0.1507 - mse: 0.1721\n","Epoch 151: val_loss improved from 0.17223 to 0.17213, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1717 - accuracy: 0.1505 - mse: 0.1722 - val_loss: 0.1721 - val_accuracy: 0.1505 - val_mse: 0.1721\n","Epoch 152/200\n","468/486 [===========================>..] - ETA: 0s - loss: 0.1718 - accuracy: 0.1506 - mse: 0.1721\n","Epoch 152: val_loss improved from 0.17213 to 0.17203, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1717 - accuracy: 0.1505 - mse: 0.1721 - val_loss: 0.1720 - val_accuracy: 0.1505 - val_mse: 0.1720\n","Epoch 153/200\n","466/486 [===========================>..] - ETA: 0s - loss: 0.1717 - accuracy: 0.1507 - mse: 0.1720\n","Epoch 153: val_loss improved from 0.17203 to 0.17193, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1716 - accuracy: 0.1505 - mse: 0.1720 - val_loss: 0.1719 - val_accuracy: 0.1505 - val_mse: 0.1719\n","Epoch 154/200\n","471/486 [============================>.] - ETA: 0s - loss: 0.1715 - accuracy: 0.1507 - mse: 0.1718\n","Epoch 154: val_loss improved from 0.17193 to 0.17183, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1715 - accuracy: 0.1505 - mse: 0.1719 - val_loss: 0.1718 - val_accuracy: 0.1505 - val_mse: 0.1718\n","Epoch 155/200\n","481/486 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.1504 - mse: 0.1718\n","Epoch 155: val_loss improved from 0.17183 to 0.17173, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1715 - accuracy: 0.1505 - mse: 0.1718 - val_loss: 0.1717 - val_accuracy: 0.1505 - val_mse: 0.1717\n","Epoch 156/200\n","483/486 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.1505 - mse: 0.1717\n","Epoch 156: val_loss improved from 0.17173 to 0.17163, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1714 - accuracy: 0.1505 - mse: 0.1717 - val_loss: 0.1716 - val_accuracy: 0.1505 - val_mse: 0.1716\n","Epoch 157/200\n","485/486 [============================>.] - ETA: 0s - loss: 0.1713 - accuracy: 0.1503 - mse: 0.1716\n","Epoch 157: val_loss improved from 0.17163 to 0.17154, saving model to pesos_1.h5\n","486/486 [==============================] - 3s 7ms/step - loss: 0.1713 - accuracy: 0.1505 - mse: 0.1716 - val_loss: 0.1715 - val_accuracy: 0.1505 - val_mse: 0.1715\n","Epoch 158/200\n","483/486 [============================>.] - ETA: 0s - loss: 0.1713 - accuracy: 0.1505 - mse: 0.1715\n","Epoch 158: val_loss improved from 0.17154 to 0.17144, saving model to pesos_1.h5\n","486/486 [==============================] - 4s 7ms/step - loss: 0.1713 - accuracy: 0.1505 - mse: 0.1715 - val_loss: 0.1714 - val_accuracy: 0.1505 - val_mse: 0.1714\n","Epoch 159/200\n","483/486 [============================>.] - ETA: 0s - loss: 0.1712 - accuracy: 0.1505 - mse: 0.1714\n","Epoch 159: val_loss improved from 0.17144 to 0.17134, saving model to pesos_1.h5\n","486/486 [==============================] - 4s 8ms/step - loss: 0.1712 - accuracy: 0.1505 - mse: 0.1714 - val_loss: 0.1713 - val_accuracy: 0.1505 - val_mse: 0.1713\n","Epoch 160/200\n","481/486 [============================>.] - ETA: 0s - loss: 0.1711 - accuracy: 0.1504 - mse: 0.1713\n","Epoch 160: val_loss improved from 0.17134 to 0.17125, saving model to pesos_1.h5\n","486/486 [==============================] - 4s 9ms/step - loss: 0.1712 - accuracy: 0.1505 - mse: 0.1713 - val_loss: 0.1712 - val_accuracy: 0.1505 - val_mse: 0.1712\n","Epoch 161/200\n","484/486 [============================>.] - ETA: 0s - loss: 0.1711 - accuracy: 0.1503 - mse: 0.1712\n","Epoch 161: val_loss improved from 0.17125 to 0.17115, saving model to pesos_1.h5\n","486/486 [==============================] - 4s 8ms/step - loss: 0.1711 - accuracy: 0.1505 - mse: 0.1712 - val_loss: 0.1712 - val_accuracy: 0.1505 - val_mse: 0.1712\n","Epoch 162/200\n","469/486 [===========================>..] - ETA: 0s - loss: 0.1711 - accuracy: 0.1508 - mse: 0.1711\n","Epoch 162: val_loss improved from 0.17115 to 0.17105, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1710 - accuracy: 0.1505 - mse: 0.1711 - val_loss: 0.1711 - val_accuracy: 0.1505 - val_mse: 0.1711\n","Epoch 163/200\n","477/486 [============================>.] - ETA: 0s - loss: 0.1709 - accuracy: 0.1504 - mse: 0.1710\n","Epoch 163: val_loss improved from 0.17105 to 0.17096, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1710 - accuracy: 0.1505 - mse: 0.1710 - val_loss: 0.1710 - val_accuracy: 0.1505 - val_mse: 0.1710\n","Epoch 164/200\n","478/486 [============================>.] - ETA: 0s - loss: 0.1708 - accuracy: 0.1505 - mse: 0.1709\n","Epoch 164: val_loss improved from 0.17096 to 0.17087, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1709 - accuracy: 0.1505 - mse: 0.1709 - val_loss: 0.1709 - val_accuracy: 0.1505 - val_mse: 0.1709\n","Epoch 165/200\n","486/486 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.1505 - mse: 0.1708\n","Epoch 165: val_loss improved from 0.17087 to 0.17077, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1708 - accuracy: 0.1505 - mse: 0.1708 - val_loss: 0.1708 - val_accuracy: 0.1505 - val_mse: 0.1708\n","Epoch 166/200\n","462/486 [===========================>..] - ETA: 0s - loss: 0.1710 - accuracy: 0.1508 - mse: 0.1707\n","Epoch 166: val_loss improved from 0.17077 to 0.17068, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1708 - accuracy: 0.1505 - mse: 0.1707 - val_loss: 0.1707 - val_accuracy: 0.1505 - val_mse: 0.1707\n","Epoch 167/200\n","473/486 [============================>.] - ETA: 0s - loss: 0.1707 - accuracy: 0.1506 - mse: 0.1706\n","Epoch 167: val_loss improved from 0.17068 to 0.17059, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1707 - accuracy: 0.1505 - mse: 0.1706 - val_loss: 0.1706 - val_accuracy: 0.1505 - val_mse: 0.1706\n","Epoch 168/200\n","475/486 [============================>.] - ETA: 0s - loss: 0.1706 - accuracy: 0.1504 - mse: 0.1705\n","Epoch 168: val_loss improved from 0.17059 to 0.17049, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1706 - accuracy: 0.1505 - mse: 0.1706 - val_loss: 0.1705 - val_accuracy: 0.1505 - val_mse: 0.1705\n","Epoch 169/200\n","481/486 [============================>.] - ETA: 0s - loss: 0.1705 - accuracy: 0.1504 - mse: 0.1704\n","Epoch 169: val_loss improved from 0.17049 to 0.17040, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1706 - accuracy: 0.1505 - mse: 0.1705 - val_loss: 0.1704 - val_accuracy: 0.1505 - val_mse: 0.1704\n","Epoch 170/200\n","483/486 [============================>.] - ETA: 0s - loss: 0.1705 - accuracy: 0.1505 - mse: 0.1704\n","Epoch 170: val_loss improved from 0.17040 to 0.17031, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1705 - accuracy: 0.1505 - mse: 0.1704 - val_loss: 0.1703 - val_accuracy: 0.1505 - val_mse: 0.1703\n","Epoch 171/200\n","478/486 [============================>.] - ETA: 0s - loss: 0.1704 - accuracy: 0.1505 - mse: 0.1702\n","Epoch 171: val_loss improved from 0.17031 to 0.17022, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1705 - accuracy: 0.1505 - mse: 0.1703 - val_loss: 0.1702 - val_accuracy: 0.1505 - val_mse: 0.1702\n","Epoch 172/200\n","478/486 [============================>.] - ETA: 0s - loss: 0.1703 - accuracy: 0.1505 - mse: 0.1701\n","Epoch 172: val_loss improved from 0.17022 to 0.17013, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1704 - accuracy: 0.1505 - mse: 0.1702 - val_loss: 0.1701 - val_accuracy: 0.1505 - val_mse: 0.1701\n","Epoch 173/200\n","464/486 [===========================>..] - ETA: 0s - loss: 0.1705 - accuracy: 0.1509 - mse: 0.1701\n","Epoch 173: val_loss improved from 0.17013 to 0.17004, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1703 - accuracy: 0.1505 - mse: 0.1701 - val_loss: 0.1700 - val_accuracy: 0.1505 - val_mse: 0.1700\n","Epoch 174/200\n","474/486 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.1505 - mse: 0.1700\n","Epoch 174: val_loss improved from 0.17004 to 0.16995, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1703 - accuracy: 0.1505 - mse: 0.1700 - val_loss: 0.1699 - val_accuracy: 0.1505 - val_mse: 0.1699\n","Epoch 175/200\n","483/486 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.1505 - mse: 0.1699\n","Epoch 175: val_loss improved from 0.16995 to 0.16986, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1702 - accuracy: 0.1505 - mse: 0.1699 - val_loss: 0.1699 - val_accuracy: 0.1505 - val_mse: 0.1699\n","Epoch 176/200\n","477/486 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.1504 - mse: 0.1698\n","Epoch 176: val_loss improved from 0.16986 to 0.16977, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1702 - accuracy: 0.1505 - mse: 0.1698 - val_loss: 0.1698 - val_accuracy: 0.1505 - val_mse: 0.1698\n","Epoch 177/200\n","477/486 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.1504 - mse: 0.1697\n","Epoch 177: val_loss improved from 0.16977 to 0.16968, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1701 - accuracy: 0.1505 - mse: 0.1697 - val_loss: 0.1697 - val_accuracy: 0.1505 - val_mse: 0.1697\n","Epoch 178/200\n","484/486 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.1503 - mse: 0.1697\n","Epoch 178: val_loss improved from 0.16968 to 0.16959, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1700 - accuracy: 0.1505 - mse: 0.1697 - val_loss: 0.1696 - val_accuracy: 0.1505 - val_mse: 0.1696\n","Epoch 179/200\n","472/486 [============================>.] - ETA: 0s - loss: 0.1699 - accuracy: 0.1507 - mse: 0.1695\n","Epoch 179: val_loss improved from 0.16959 to 0.16951, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1700 - accuracy: 0.1505 - mse: 0.1696 - val_loss: 0.1695 - val_accuracy: 0.1505 - val_mse: 0.1695\n","Epoch 180/200\n","466/486 [===========================>..] - ETA: 0s - loss: 0.1700 - accuracy: 0.1507 - mse: 0.1695\n","Epoch 180: val_loss improved from 0.16951 to 0.16942, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1699 - accuracy: 0.1505 - mse: 0.1695 - val_loss: 0.1694 - val_accuracy: 0.1505 - val_mse: 0.1694\n","Epoch 181/200\n","466/486 [===========================>..] - ETA: 0s - loss: 0.1700 - accuracy: 0.1507 - mse: 0.1694\n","Epoch 181: val_loss improved from 0.16942 to 0.16933, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1699 - accuracy: 0.1505 - mse: 0.1694 - val_loss: 0.1693 - val_accuracy: 0.1505 - val_mse: 0.1693\n","Epoch 182/200\n","478/486 [============================>.] - ETA: 0s - loss: 0.1697 - accuracy: 0.1505 - mse: 0.1693\n","Epoch 182: val_loss improved from 0.16933 to 0.16925, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1698 - accuracy: 0.1505 - mse: 0.1693 - val_loss: 0.1692 - val_accuracy: 0.1505 - val_mse: 0.1692\n","Epoch 183/200\n","472/486 [============================>.] - ETA: 0s - loss: 0.1697 - accuracy: 0.1507 - mse: 0.1692\n","Epoch 183: val_loss improved from 0.16925 to 0.16916, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1698 - accuracy: 0.1505 - mse: 0.1692 - val_loss: 0.1692 - val_accuracy: 0.1505 - val_mse: 0.1692\n","Epoch 184/200\n","474/486 [============================>.] - ETA: 0s - loss: 0.1697 - accuracy: 0.1505 - mse: 0.1691\n","Epoch 184: val_loss improved from 0.16916 to 0.16908, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1697 - accuracy: 0.1505 - mse: 0.1691 - val_loss: 0.1691 - val_accuracy: 0.1505 - val_mse: 0.1691\n","Epoch 185/200\n","481/486 [============================>.] - ETA: 0s - loss: 0.1696 - accuracy: 0.1504 - mse: 0.1690\n","Epoch 185: val_loss improved from 0.16908 to 0.16899, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1696 - accuracy: 0.1505 - mse: 0.1690 - val_loss: 0.1690 - val_accuracy: 0.1505 - val_mse: 0.1690\n","Epoch 186/200\n","485/486 [============================>.] - ETA: 0s - loss: 0.1696 - accuracy: 0.1503 - mse: 0.1690\n","Epoch 186: val_loss improved from 0.16899 to 0.16891, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1696 - accuracy: 0.1505 - mse: 0.1690 - val_loss: 0.1689 - val_accuracy: 0.1505 - val_mse: 0.1689\n","Epoch 187/200\n","471/486 [============================>.] - ETA: 0s - loss: 0.1695 - accuracy: 0.1507 - mse: 0.1688\n","Epoch 187: val_loss improved from 0.16891 to 0.16882, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1695 - accuracy: 0.1505 - mse: 0.1689 - val_loss: 0.1688 - val_accuracy: 0.1505 - val_mse: 0.1688\n","Epoch 188/200\n","470/486 [============================>.] - ETA: 0s - loss: 0.1695 - accuracy: 0.1506 - mse: 0.1687\n","Epoch 188: val_loss improved from 0.16882 to 0.16874, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1695 - accuracy: 0.1505 - mse: 0.1688 - val_loss: 0.1687 - val_accuracy: 0.1505 - val_mse: 0.1687\n","Epoch 189/200\n","471/486 [============================>.] - ETA: 0s - loss: 0.1694 - accuracy: 0.1507 - mse: 0.1687\n","Epoch 189: val_loss improved from 0.16874 to 0.16866, saving model to pesos_1.h5\n","486/486 [==============================] - 1s 3ms/step - loss: 0.1694 - accuracy: 0.1505 - mse: 0.1687 - val_loss: 0.1687 - val_accuracy: 0.1505 - val_mse: 0.1687\n","Epoch 190/200\n","470/486 [============================>.] - ETA: 0s - loss: 0.1694 - accuracy: 0.1506 - mse: 0.1686\n","Epoch 190: val_loss improved from 0.16866 to 0.16857, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1694 - accuracy: 0.1505 - mse: 0.1686 - val_loss: 0.1686 - val_accuracy: 0.1505 - val_mse: 0.1686\n","Epoch 191/200\n","482/486 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.1503 - mse: 0.1685\n","Epoch 191: val_loss improved from 0.16857 to 0.16849, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1693 - accuracy: 0.1505 - mse: 0.1685 - val_loss: 0.1685 - val_accuracy: 0.1505 - val_mse: 0.1685\n","Epoch 192/200\n","485/486 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.1503 - mse: 0.1685\n","Epoch 192: val_loss improved from 0.16849 to 0.16841, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1693 - accuracy: 0.1505 - mse: 0.1685 - val_loss: 0.1684 - val_accuracy: 0.1505 - val_mse: 0.1684\n","Epoch 193/200\n","481/486 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.1504 - mse: 0.1684\n","Epoch 193: val_loss improved from 0.16841 to 0.16833, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1692 - accuracy: 0.1505 - mse: 0.1684 - val_loss: 0.1683 - val_accuracy: 0.1505 - val_mse: 0.1683\n","Epoch 194/200\n","486/486 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.1505 - mse: 0.1683\n","Epoch 194: val_loss improved from 0.16833 to 0.16825, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1691 - accuracy: 0.1505 - mse: 0.1683 - val_loss: 0.1682 - val_accuracy: 0.1505 - val_mse: 0.1682\n","Epoch 195/200\n","469/486 [===========================>..] - ETA: 0s - loss: 0.1692 - accuracy: 0.1508 - mse: 0.1682\n","Epoch 195: val_loss improved from 0.16825 to 0.16817, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 4ms/step - loss: 0.1691 - accuracy: 0.1505 - mse: 0.1682 - val_loss: 0.1682 - val_accuracy: 0.1505 - val_mse: 0.1682\n","Epoch 196/200\n","469/486 [===========================>..] - ETA: 0s - loss: 0.1691 - accuracy: 0.1508 - mse: 0.1681\n","Epoch 196: val_loss improved from 0.16817 to 0.16809, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1690 - accuracy: 0.1505 - mse: 0.1681 - val_loss: 0.1681 - val_accuracy: 0.1505 - val_mse: 0.1681\n","Epoch 197/200\n","480/486 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.1505 - mse: 0.1680\n","Epoch 197: val_loss improved from 0.16809 to 0.16801, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1690 - accuracy: 0.1505 - mse: 0.1681 - val_loss: 0.1680 - val_accuracy: 0.1505 - val_mse: 0.1680\n","Epoch 198/200\n","472/486 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.1507 - mse: 0.1679\n","Epoch 198: val_loss improved from 0.16801 to 0.16793, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1689 - accuracy: 0.1505 - mse: 0.1680 - val_loss: 0.1679 - val_accuracy: 0.1505 - val_mse: 0.1679\n","Epoch 199/200\n","464/486 [===========================>..] - ETA: 0s - loss: 0.1691 - accuracy: 0.1509 - mse: 0.1679\n","Epoch 199: val_loss improved from 0.16793 to 0.16785, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1689 - accuracy: 0.1505 - mse: 0.1679 - val_loss: 0.1679 - val_accuracy: 0.1505 - val_mse: 0.1679\n","Epoch 200/200\n","465/486 [===========================>..] - ETA: 0s - loss: 0.1690 - accuracy: 0.1507 - mse: 0.1678\n","Epoch 200: val_loss improved from 0.16785 to 0.16778, saving model to pesos_1.h5\n","486/486 [==============================] - 2s 3ms/step - loss: 0.1688 - accuracy: 0.1505 - mse: 0.1678 - val_loss: 0.1678 - val_accuracy: 0.1505 - val_mse: 0.1678\n"]}],"source":["#ENTRENAMIENTO DEL MODELO\n","opcion=2\n","if opcion==1:\n","  callbacks_list=[early_stopping]\n","\n","historico = model.fit(X_train2, y_train_encoded2,\n","                      epochs = epochs_H, batch_size = batch_size_H, verbose=1, validation_data=(X_Validation,y_Validation), shuffle=False,\n","                      callbacks=callbacks_list,\n","                      class_weight=class_weights)"]},{"cell_type":"markdown","metadata":{"id":"7rLz1YP8WPdE"},"source":["## Análisis de los resultados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltRipRM9WPrd"},"outputs":[],"source":["#GUARDAR RESULTADOS Y MODELOS EN FICHEROS\n","#CUANDO SE UTILIZA CHECKPOINT, YA ESTÁ GUARDADO EL MODELO\n","#CUANDO SE UTILIZA EARLY_STOPING, REALEMNTE EL MODELO YA ESTÁ EN \"MODEL\", PERO SI SE QUIERE GUARDAR EN FICHERO SE DEBE UTILIZAR MODEL.SAVE\n","\n","np.savetxt(path+'historicoTrainLoss.txt',historico.history['loss'])\n","np.savetxt(path+'historicoValLoss.txt',historico.history['val_loss'])\n","np.savetxt(path+'historicoTrainAcc.txt',historico.history['accuracy'])\n","np.savetxt(path+'historicoValAcc.txt',historico.history['val_accuracy'])\n","\n","if opcion==1:\n","    model.save(path+'model_early_stoping')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1702666758963,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"B5Fiqe3BWpC8","outputId":"29561371-6cdc-4f3a-e05a-c8ad8fadea32"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.16031897068023682\n","0.16101780533790588\n","0.24454878270626068\n","0.2508361339569092\n"]}],"source":["#LOSS y ACCURACY DE TRAIN Y VALIDACIÓN CUANDO SE UTILIZA EARLY_STOPPING\n","final_epoch = len(historico.history['loss'])\n","print(historico.history['loss'][final_epoch-1])\n","print(historico.history['val_loss'][final_epoch-1])\n","print(historico.history['accuracy'][final_epoch-1])\n","print(historico.history['val_accuracy'][final_epoch-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1702672816514,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"uXjQuIuQWqVT","outputId":"0f16abd1-f0e6-4113-9357-d571f798ef7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["199\n","0.16888286173343658\n","0.1678536981344223\n","0.15051135420799255\n","0.15050166845321655\n","0.16790823638439178\n","0.1678536981344223\n"]}],"source":["#MEJOR ÉPOCA, LOSS , ACCURACY Y MSE DE TRAIN Y VALIDACIÓN CUANDO SE UTILIZA CHECKPOINT\n","epochs_stop = np.where(historico.history['val_loss'] == np.min(historico.history['val_loss']))\n","final_epoch = epochs_stop[0][0]\n","print(final_epoch)\n","print(historico.history['loss'][final_epoch-1])\n","print(historico.history['val_loss'][final_epoch-1])\n","print(historico.history['accuracy'][final_epoch-1])\n","print(historico.history['val_accuracy'][final_epoch-1])\n","print(historico.history['mse'][final_epoch-1])\n","print(historico.history['val_mse'][final_epoch-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":927},"executionInfo":{"elapsed":837,"status":"ok","timestamp":1702672817350,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"7chN4bmxf5ic","outputId":"45a17961-7d55-4903-b63c-536f584d844a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["## plots de evolución de loss y accuracy\n","from matplotlib import pyplot as plt\n","plt.plot(historico.history['accuracy'])\n","plt.plot(historico.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()\n","\n","plt.plot(historico.history['loss'])\n","plt.plot(historico.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","source":["# Mejor modelo"],"metadata":{"id":"dp3UevGXBFVk"}},{"cell_type":"markdown","source":["Uso de checkpoint para obtener los mejores pesos posteriormente"],"metadata":{"id":"FrHNcDOoBo71"}},{"cell_type":"code","source":["path= \"/content/\"\n","checkpoint = ModelCheckpoint(\n","                            'pesos.h5',  # nombre del archivo para guardar los pesos\n","                             monitor='loss',  # métrica para monitorear (puede ser 'val_loss', 'val_accuracy', etc.)\n","                             save_best_only=True,  # guarda solo si la métrica mejora\n","                             mode='min',  # 'max' si quieres maximizar la métrica, 'min' si quieres minimizar\n","                             save_weights_only=True,  # guarda solo los pesos, no el modelo completo\n","                             verbose=1)\n","callbacks_list = [checkpoint]"],"metadata":{"id":"DVQDklfuBHJT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pesos balanceados"],"metadata":{"id":"Kph4aWfTBnSX"}},{"cell_type":"code","source":["# Pesos originales :  [0.5265707  1.07442985 0.88385446 1.32880342 3.49370787]\n","\n","# Encuentra el peso mínimo\n","min_weight = min(class_weights.values())\n","\n","# Normaliza los pesos dividiendo por el peso mínimo\n","balanced_weights = {k: v / min_weight for k, v in class_weights.items()}\n","\n","# Muestra los pesos equilibrados\n","print(balanced_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TswVgksLBUH3","executionInfo":{"status":"ok","timestamp":1702671689400,"user_tz":-60,"elapsed":228,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"}},"outputId":"013a12a6-47f1-487e-dd04-70182cdbc1c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 1.0, 1: 2.0404284727021427, 2: 1.6785105173393975, 3: 2.523504273504274, 4: 6.634831460674158}\n"]}]},{"cell_type":"markdown","source":["Definicion del modelo"],"metadata":{"id":"QcqqnTeyBsY_"}},{"cell_type":"code","source":["# Definir forma de la entrada de la red\n","input_shape=(X_train2.shape [1],) # utilizamos los datos de entrenamiento para definir la tupla\n","\n","# Definir la salida de la red, la ultima capa debera tener una neurona por cada clase\n","num_classes = y_train_encoded2.shape[1] # definimos el numero de clases que queremos que tenga la salida\n","\n","#DEFINICIÓN DEL MODELO\n","model = Sequential()\n","\n","# Capa de entrada del modelo\n","model.add(Dense(50, input_shape=input_shape, activation='sigmoid'))\n","\n","#Capas ocultas:\n","model.add(Dense(50, activation='sigmoid'))\n","model.add(Dense(50, activation='sigmoid'))\n","\n","# Capa de salida del modelo\n","model.add(Dense(num_classes, activation='softmax'))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jXcV0rrNBsm9","executionInfo":{"status":"ok","timestamp":1702671705394,"user_tz":-60,"elapsed":265,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"}},"outputId":"d87ff34f-dd1d-4e42-ef5b-141da8f4935d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_22\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_68 (Dense)            (None, 50)                550       \n","                                                                 \n"," dense_69 (Dense)            (None, 50)                2550      \n","                                                                 \n"," dense_70 (Dense)            (None, 50)                2550      \n","                                                                 \n"," dense_71 (Dense)            (None, 5)                 255       \n","                                                                 \n","=================================================================\n","Total params: 5905 (23.07 KB)\n","Trainable params: 5905 (23.07 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Hiperparametros"],"metadata":{"id":"-mzbOjuEBzzp"}},{"cell_type":"code","source":["# Hiperparametros\n","lr = 0.00003 # razon de aprendizaje\n","momento=0\n","epochs_H = 200 # numero de ciclos que se quiere realizar en el entrenamiento\n","batch_size_H = 32\n","optimizador=tf.keras.optimizers.SGD(learning_rate=lr, momentum=momento)\n","#lr=1e-3\n","#optimizador=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n","#rho=0.9\n","#lr=0.001\n","#optimizador=tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9),"],"metadata":{"id":"52nSoAd7Bz9-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compilación del modelo"],"metadata":{"id":"AwPvmaL3B5sU"}},{"cell_type":"code","source":["model.compile(\n","    optimizer = optimizador,\n","    #optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n","    #optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n","    loss='mean_squared_error',\n","    #loss='binary_crossentropy',\n","    metrics=['accuracy','mse']\n",")"],"metadata":{"id":"HBZNJDOWB6Ap"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Entrenamiento del modelo"],"metadata":{"id":"0w-k9rsSCBbY"}},{"cell_type":"code","source":["#ENTRENAMIENTO DEL MODELO SIN VALIDACION\n","opcion=2\n","if opcion==1:\n","  callbacks_list=[early_stopping]\n","\n","historico = model.fit(X_train, y_train_encoded,\n","                      epochs = epochs_H, batch_size = batch_size_H, verbose=1, shuffle=False,\n","                      callbacks=callbacks_list,\n","                      class_weight=class_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhWlZIriCBs_","executionInfo":{"status":"ok","timestamp":1702671999499,"user_tz":-60,"elapsed":289653,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"}},"outputId":"c032318f-6967-47ea-e8a8-54f47fe4aeeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","593/608 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.2270 - mse: 0.1779\n","Epoch 1: loss improved from inf to 0.18937, saving model to pesos.h5\n","608/608 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.2263 - mse: 0.1780\n","Epoch 2/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1889 - accuracy: 0.2264 - mse: 0.1775\n","Epoch 2: loss improved from 0.18937 to 0.18882, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1888 - accuracy: 0.2263 - mse: 0.1775\n","Epoch 3/200\n","583/608 [===========================>..] - ETA: 0s - loss: 0.1876 - accuracy: 0.2291 - mse: 0.1765\n","Epoch 3: loss improved from 0.18882 to 0.18829, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1883 - accuracy: 0.2263 - mse: 0.1769\n","Epoch 4/200\n","593/608 [============================>.] - ETA: 0s - loss: 0.1881 - accuracy: 0.2270 - mse: 0.1763\n","Epoch 4: loss improved from 0.18829 to 0.18777, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1878 - accuracy: 0.2263 - mse: 0.1764\n","Epoch 5/200\n","606/608 [============================>.] - ETA: 0s - loss: 0.1874 - accuracy: 0.2258 - mse: 0.1759\n","Epoch 5: loss improved from 0.18777 to 0.18726, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1873 - accuracy: 0.2263 - mse: 0.1759\n","Epoch 6/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1870 - accuracy: 0.2268 - mse: 0.1753\n","Epoch 6: loss improved from 0.18726 to 0.18676, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1868 - accuracy: 0.2263 - mse: 0.1754\n","Epoch 7/200\n","589/608 [============================>.] - ETA: 0s - loss: 0.1858 - accuracy: 0.2279 - mse: 0.1746\n","Epoch 7: loss improved from 0.18676 to 0.18628, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.2263 - mse: 0.1749\n","Epoch 8/200\n","582/608 [===========================>..] - ETA: 0s - loss: 0.1853 - accuracy: 0.2294 - mse: 0.1739\n","Epoch 8: loss improved from 0.18628 to 0.18580, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1858 - accuracy: 0.2263 - mse: 0.1744\n","Epoch 9/200\n","607/608 [============================>.] - ETA: 0s - loss: 0.1854 - accuracy: 0.2264 - mse: 0.1739\n","Epoch 9: loss improved from 0.18580 to 0.18534, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.2263 - mse: 0.1739\n","Epoch 10/200\n","591/608 [============================>.] - ETA: 0s - loss: 0.1851 - accuracy: 0.2272 - mse: 0.1733\n","Epoch 10: loss improved from 0.18534 to 0.18489, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1849 - accuracy: 0.2263 - mse: 0.1734\n","Epoch 11/200\n","597/608 [============================>.] - ETA: 0s - loss: 0.1845 - accuracy: 0.2265 - mse: 0.1729\n","Epoch 11: loss improved from 0.18489 to 0.18445, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1845 - accuracy: 0.2263 - mse: 0.1730\n","Epoch 12/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.2271 - mse: 0.1724\n","Epoch 12: loss improved from 0.18445 to 0.18402, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1840 - accuracy: 0.2263 - mse: 0.1725\n","Epoch 13/200\n","585/608 [===========================>..] - ETA: 0s - loss: 0.1830 - accuracy: 0.2288 - mse: 0.1717\n","Epoch 13: loss improved from 0.18402 to 0.18361, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1836 - accuracy: 0.2263 - mse: 0.1721\n","Epoch 14/200\n","586/608 [===========================>..] - ETA: 0s - loss: 0.1826 - accuracy: 0.2286 - mse: 0.1713\n","Epoch 14: loss improved from 0.18361 to 0.18320, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1832 - accuracy: 0.2263 - mse: 0.1717\n","Epoch 15/200\n","590/608 [============================>.] - ETA: 0s - loss: 0.1822 - accuracy: 0.2276 - mse: 0.1710\n","Epoch 15: loss improved from 0.18320 to 0.18280, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1828 - accuracy: 0.2263 - mse: 0.1713\n","Epoch 16/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1823 - accuracy: 0.2271 - mse: 0.1707\n","Epoch 16: loss improved from 0.18280 to 0.18241, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1824 - accuracy: 0.2263 - mse: 0.1709\n","Epoch 17/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1823 - accuracy: 0.2268 - mse: 0.1704\n","Epoch 17: loss improved from 0.18241 to 0.18203, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1820 - accuracy: 0.2263 - mse: 0.1705\n","Epoch 18/200\n","605/608 [============================>.] - ETA: 0s - loss: 0.1818 - accuracy: 0.2261 - mse: 0.1701\n","Epoch 18: loss improved from 0.18203 to 0.18167, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.2263 - mse: 0.1701\n","Epoch 19/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1815 - accuracy: 0.2268 - mse: 0.1696\n","Epoch 19: loss improved from 0.18167 to 0.18131, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.2263 - mse: 0.1697\n","Epoch 20/200\n","604/608 [============================>.] - ETA: 0s - loss: 0.1812 - accuracy: 0.2261 - mse: 0.1694\n","Epoch 20: loss improved from 0.18131 to 0.18096, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1810 - accuracy: 0.2263 - mse: 0.1694\n","Epoch 21/200\n","606/608 [============================>.] - ETA: 0s - loss: 0.1807 - accuracy: 0.2258 - mse: 0.1691\n","Epoch 21: loss improved from 0.18096 to 0.18061, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1806 - accuracy: 0.2263 - mse: 0.1690\n","Epoch 22/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1804 - accuracy: 0.2264 - mse: 0.1687\n","Epoch 22: loss improved from 0.18061 to 0.18028, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1803 - accuracy: 0.2263 - mse: 0.1687\n","Epoch 23/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1801 - accuracy: 0.2269 - mse: 0.1682\n","Epoch 23: loss improved from 0.18028 to 0.17996, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.2263 - mse: 0.1683\n","Epoch 24/200\n","591/608 [============================>.] - ETA: 0s - loss: 0.1799 - accuracy: 0.2272 - mse: 0.1679\n","Epoch 24: loss improved from 0.17996 to 0.17964, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.2263 - mse: 0.1680\n","Epoch 25/200\n","607/608 [============================>.] - ETA: 0s - loss: 0.1794 - accuracy: 0.2264 - mse: 0.1677\n","Epoch 25: loss improved from 0.17964 to 0.17933, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1793 - accuracy: 0.2263 - mse: 0.1677\n","Epoch 26/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1793 - accuracy: 0.2268 - mse: 0.1673\n","Epoch 26: loss improved from 0.17933 to 0.17903, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1790 - accuracy: 0.2263 - mse: 0.1674\n","Epoch 27/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1788 - accuracy: 0.2264 - mse: 0.1671\n","Epoch 27: loss improved from 0.17903 to 0.17874, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1787 - accuracy: 0.2263 - mse: 0.1671\n","Epoch 28/200\n","594/608 [============================>.] - ETA: 0s - loss: 0.1787 - accuracy: 0.2269 - mse: 0.1667\n","Epoch 28: loss improved from 0.17874 to 0.17845, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.2263 - mse: 0.1668\n","Epoch 29/200\n","585/608 [===========================>..] - ETA: 0s - loss: 0.1776 - accuracy: 0.2288 - mse: 0.1662\n","Epoch 29: loss improved from 0.17845 to 0.17818, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.2263 - mse: 0.1665\n","Epoch 30/200\n","608/608 [==============================] - ETA: 0s - loss: 0.1779 - accuracy: 0.2263 - mse: 0.1662\n","Epoch 30: loss improved from 0.17818 to 0.17790, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1779 - accuracy: 0.2263 - mse: 0.1662\n","Epoch 31/200\n","592/608 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.2274 - mse: 0.1658\n","Epoch 31: loss improved from 0.17790 to 0.17764, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1776 - accuracy: 0.2263 - mse: 0.1659\n","Epoch 32/200\n","597/608 [============================>.] - ETA: 0s - loss: 0.1774 - accuracy: 0.2265 - mse: 0.1656\n","Epoch 32: loss improved from 0.17764 to 0.17738, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1774 - accuracy: 0.2263 - mse: 0.1657\n","Epoch 33/200\n","608/608 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.2263 - mse: 0.1654\n","Epoch 33: loss improved from 0.17738 to 0.17713, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1771 - accuracy: 0.2263 - mse: 0.1654\n","Epoch 34/200\n","586/608 [===========================>..] - ETA: 0s - loss: 0.1763 - accuracy: 0.2286 - mse: 0.1648\n","Epoch 34: loss improved from 0.17713 to 0.17688, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.2263 - mse: 0.1652\n","Epoch 35/200\n","598/608 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.2267 - mse: 0.1648\n","Epoch 35: loss improved from 0.17688 to 0.17665, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1766 - accuracy: 0.2263 - mse: 0.1649\n","Epoch 36/200\n","591/608 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.2272 - mse: 0.1646\n","Epoch 36: loss improved from 0.17665 to 0.17641, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.2263 - mse: 0.1647\n","Epoch 37/200\n","597/608 [============================>.] - ETA: 0s - loss: 0.1762 - accuracy: 0.2265 - mse: 0.1644\n","Epoch 37: loss improved from 0.17641 to 0.17619, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.2263 - mse: 0.1644\n","Epoch 38/200\n","582/608 [===========================>..] - ETA: 0s - loss: 0.1756 - accuracy: 0.2294 - mse: 0.1639\n","Epoch 38: loss improved from 0.17619 to 0.17596, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.2263 - mse: 0.1642\n","Epoch 39/200\n","601/608 [============================>.] - ETA: 0s - loss: 0.1757 - accuracy: 0.2268 - mse: 0.1639\n","Epoch 39: loss improved from 0.17596 to 0.17575, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.2263 - mse: 0.1640\n","Epoch 40/200\n","604/608 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.2261 - mse: 0.1638\n","Epoch 40: loss improved from 0.17575 to 0.17554, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1755 - accuracy: 0.2263 - mse: 0.1638\n","Epoch 41/200\n","593/608 [============================>.] - ETA: 0s - loss: 0.1756 - accuracy: 0.2270 - mse: 0.1635\n","Epoch 41: loss improved from 0.17554 to 0.17533, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1753 - accuracy: 0.2263 - mse: 0.1636\n","Epoch 42/200\n","597/608 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.2265 - mse: 0.1633\n","Epoch 42: loss improved from 0.17533 to 0.17513, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1751 - accuracy: 0.2263 - mse: 0.1633\n","Epoch 43/200\n","588/608 [============================>.] - ETA: 0s - loss: 0.1745 - accuracy: 0.2279 - mse: 0.1629\n","Epoch 43: loss improved from 0.17513 to 0.17493, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.2263 - mse: 0.1631\n","Epoch 44/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.2268 - mse: 0.1629\n","Epoch 44: loss improved from 0.17493 to 0.17474, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1747 - accuracy: 0.2263 - mse: 0.1629\n","Epoch 45/200\n","589/608 [============================>.] - ETA: 0s - loss: 0.1741 - accuracy: 0.2279 - mse: 0.1625\n","Epoch 45: loss improved from 0.17474 to 0.17455, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1746 - accuracy: 0.2263 - mse: 0.1628\n","Epoch 46/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1743 - accuracy: 0.2271 - mse: 0.1624\n","Epoch 46: loss improved from 0.17455 to 0.17437, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1744 - accuracy: 0.2263 - mse: 0.1626\n","Epoch 47/200\n","600/608 [============================>.] - ETA: 0s - loss: 0.1742 - accuracy: 0.2270 - mse: 0.1623\n","Epoch 47: loss improved from 0.17437 to 0.17419, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1742 - accuracy: 0.2263 - mse: 0.1624\n","Epoch 48/200\n","591/608 [============================>.] - ETA: 0s - loss: 0.1743 - accuracy: 0.2272 - mse: 0.1621\n","Epoch 48: loss improved from 0.17419 to 0.17402, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1740 - accuracy: 0.2263 - mse: 0.1622\n","Epoch 49/200\n","588/608 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.2279 - mse: 0.1618\n","Epoch 49: loss improved from 0.17402 to 0.17385, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1738 - accuracy: 0.2263 - mse: 0.1620\n","Epoch 50/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1736 - accuracy: 0.2271 - mse: 0.1617\n","Epoch 50: loss improved from 0.17385 to 0.17368, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1737 - accuracy: 0.2263 - mse: 0.1619\n","Epoch 51/200\n","601/608 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.2268 - mse: 0.1616\n","Epoch 51: loss improved from 0.17368 to 0.17352, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1735 - accuracy: 0.2263 - mse: 0.1617\n","Epoch 52/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.2264 - mse: 0.1615\n","Epoch 52: loss improved from 0.17352 to 0.17336, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1734 - accuracy: 0.2263 - mse: 0.1615\n","Epoch 53/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.2271 - mse: 0.1613\n","Epoch 53: loss improved from 0.17336 to 0.17321, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1732 - accuracy: 0.2263 - mse: 0.1614\n","Epoch 54/200\n","603/608 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.2263 - mse: 0.1612\n","Epoch 54: loss improved from 0.17321 to 0.17306, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1731 - accuracy: 0.2263 - mse: 0.1612\n","Epoch 55/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.2269 - mse: 0.1610\n","Epoch 55: loss improved from 0.17306 to 0.17291, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1729 - accuracy: 0.2263 - mse: 0.1611\n","Epoch 56/200\n","590/608 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.2276 - mse: 0.1607\n","Epoch 56: loss improved from 0.17291 to 0.17276, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1728 - accuracy: 0.2263 - mse: 0.1609\n","Epoch 57/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1725 - accuracy: 0.2271 - mse: 0.1607\n","Epoch 57: loss improved from 0.17276 to 0.17262, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1726 - accuracy: 0.2263 - mse: 0.1608\n","Epoch 58/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.2268 - mse: 0.1606\n","Epoch 58: loss improved from 0.17262 to 0.17248, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1725 - accuracy: 0.2263 - mse: 0.1606\n","Epoch 59/200\n","583/608 [===========================>..] - ETA: 0s - loss: 0.1719 - accuracy: 0.2291 - mse: 0.1602\n","Epoch 59: loss improved from 0.17248 to 0.17235, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1723 - accuracy: 0.2263 - mse: 0.1605\n","Epoch 60/200\n","603/608 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.2263 - mse: 0.1604\n","Epoch 60: loss improved from 0.17235 to 0.17222, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1722 - accuracy: 0.2263 - mse: 0.1604\n","Epoch 61/200\n","604/608 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.2261 - mse: 0.1603\n","Epoch 61: loss improved from 0.17222 to 0.17209, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1721 - accuracy: 0.2263 - mse: 0.1602\n","Epoch 62/200\n","587/608 [===========================>..] - ETA: 0s - loss: 0.1716 - accuracy: 0.2282 - mse: 0.1599\n","Epoch 62: loss improved from 0.17209 to 0.17196, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1720 - accuracy: 0.2263 - mse: 0.1601\n","Epoch 63/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1718 - accuracy: 0.2271 - mse: 0.1599\n","Epoch 63: loss improved from 0.17196 to 0.17184, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1718 - accuracy: 0.2263 - mse: 0.1600\n","Epoch 64/200\n","604/608 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.2261 - mse: 0.1599\n","Epoch 64: loss improved from 0.17184 to 0.17172, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1717 - accuracy: 0.2263 - mse: 0.1599\n","Epoch 65/200\n","589/608 [============================>.] - ETA: 0s - loss: 0.1712 - accuracy: 0.2279 - mse: 0.1596\n","Epoch 65: loss improved from 0.17172 to 0.17160, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1716 - accuracy: 0.2263 - mse: 0.1597\n","Epoch 66/200\n","582/608 [===========================>..] - ETA: 0s - loss: 0.1712 - accuracy: 0.2294 - mse: 0.1594\n","Epoch 66: loss improved from 0.17160 to 0.17148, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1715 - accuracy: 0.2263 - mse: 0.1596\n","Epoch 67/200\n","582/608 [===========================>..] - ETA: 0s - loss: 0.1711 - accuracy: 0.2294 - mse: 0.1593\n","Epoch 67: loss improved from 0.17148 to 0.17137, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1714 - accuracy: 0.2263 - mse: 0.1595\n","Epoch 68/200\n","603/608 [============================>.] - ETA: 0s - loss: 0.1713 - accuracy: 0.2263 - mse: 0.1594\n","Epoch 68: loss improved from 0.17137 to 0.17126, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1713 - accuracy: 0.2263 - mse: 0.1594\n","Epoch 69/200\n","598/608 [============================>.] - ETA: 0s - loss: 0.1711 - accuracy: 0.2267 - mse: 0.1592\n","Epoch 69: loss improved from 0.17126 to 0.17115, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1711 - accuracy: 0.2263 - mse: 0.1593\n","Epoch 70/200\n","603/608 [============================>.] - ETA: 0s - loss: 0.1711 - accuracy: 0.2263 - mse: 0.1592\n","Epoch 70: loss improved from 0.17115 to 0.17104, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1710 - accuracy: 0.2263 - mse: 0.1592\n","Epoch 71/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.2264 - mse: 0.1591\n","Epoch 71: loss improved from 0.17104 to 0.17093, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1709 - accuracy: 0.2263 - mse: 0.1591\n","Epoch 72/200\n","590/608 [============================>.] - ETA: 0s - loss: 0.1703 - accuracy: 0.2276 - mse: 0.1588\n","Epoch 72: loss improved from 0.17093 to 0.17083, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1708 - accuracy: 0.2263 - mse: 0.1590\n","Epoch 73/200\n","594/608 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.2269 - mse: 0.1588\n","Epoch 73: loss improved from 0.17083 to 0.17073, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1707 - accuracy: 0.2263 - mse: 0.1589\n","Epoch 74/200\n","592/608 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.2274 - mse: 0.1587\n","Epoch 74: loss improved from 0.17073 to 0.17063, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1706 - accuracy: 0.2263 - mse: 0.1588\n","Epoch 75/200\n","606/608 [============================>.] - ETA: 0s - loss: 0.1706 - accuracy: 0.2258 - mse: 0.1587\n","Epoch 75: loss improved from 0.17063 to 0.17053, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1705 - accuracy: 0.2263 - mse: 0.1587\n","Epoch 76/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1707 - accuracy: 0.2268 - mse: 0.1585\n","Epoch 76: loss improved from 0.17053 to 0.17044, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1704 - accuracy: 0.2263 - mse: 0.1586\n","Epoch 77/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1704 - accuracy: 0.2264 - mse: 0.1585\n","Epoch 77: loss improved from 0.17044 to 0.17035, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1703 - accuracy: 0.2263 - mse: 0.1585\n","Epoch 78/200\n","607/608 [============================>.] - ETA: 0s - loss: 0.1703 - accuracy: 0.2264 - mse: 0.1584\n","Epoch 78: loss improved from 0.17035 to 0.17025, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1703 - accuracy: 0.2263 - mse: 0.1584\n","Epoch 79/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.2264 - mse: 0.1583\n","Epoch 79: loss improved from 0.17025 to 0.17016, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1702 - accuracy: 0.2263 - mse: 0.1583\n","Epoch 80/200\n","597/608 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.2265 - mse: 0.1582\n","Epoch 80: loss improved from 0.17016 to 0.17008, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1701 - accuracy: 0.2263 - mse: 0.1583\n","Epoch 81/200\n","605/608 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.2261 - mse: 0.1582\n","Epoch 81: loss improved from 0.17008 to 0.16999, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1700 - accuracy: 0.2263 - mse: 0.1582\n","Epoch 82/200\n","608/608 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.2263 - mse: 0.1581\n","Epoch 82: loss improved from 0.16999 to 0.16990, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1699 - accuracy: 0.2263 - mse: 0.1581\n","Epoch 83/200\n","597/608 [============================>.] - ETA: 0s - loss: 0.1699 - accuracy: 0.2265 - mse: 0.1579\n","Epoch 83: loss improved from 0.16990 to 0.16982, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1698 - accuracy: 0.2263 - mse: 0.1580\n","Epoch 84/200\n","586/608 [===========================>..] - ETA: 0s - loss: 0.1693 - accuracy: 0.2286 - mse: 0.1577\n","Epoch 84: loss improved from 0.16982 to 0.16974, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1697 - accuracy: 0.2263 - mse: 0.1579\n","Epoch 85/200\n","598/608 [============================>.] - ETA: 0s - loss: 0.1697 - accuracy: 0.2267 - mse: 0.1578\n","Epoch 85: loss improved from 0.16974 to 0.16966, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1697 - accuracy: 0.2263 - mse: 0.1579\n","Epoch 86/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1698 - accuracy: 0.2268 - mse: 0.1577\n","Epoch 86: loss improved from 0.16966 to 0.16958, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1696 - accuracy: 0.2263 - mse: 0.1578\n","Epoch 87/200\n","597/608 [============================>.] - ETA: 0s - loss: 0.1696 - accuracy: 0.2265 - mse: 0.1576\n","Epoch 87: loss improved from 0.16958 to 0.16950, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1695 - accuracy: 0.2263 - mse: 0.1577\n","Epoch 88/200\n","600/608 [============================>.] - ETA: 0s - loss: 0.1694 - accuracy: 0.2270 - mse: 0.1576\n","Epoch 88: loss improved from 0.16950 to 0.16942, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1694 - accuracy: 0.2263 - mse: 0.1576\n","Epoch 89/200\n","605/608 [============================>.] - ETA: 0s - loss: 0.1695 - accuracy: 0.2261 - mse: 0.1576\n","Epoch 89: loss improved from 0.16942 to 0.16935, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1693 - accuracy: 0.2263 - mse: 0.1576\n","Epoch 90/200\n","585/608 [===========================>..] - ETA: 0s - loss: 0.1689 - accuracy: 0.2288 - mse: 0.1573\n","Epoch 90: loss improved from 0.16935 to 0.16927, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1693 - accuracy: 0.2263 - mse: 0.1575\n","Epoch 91/200\n","589/608 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.2279 - mse: 0.1573\n","Epoch 91: loss improved from 0.16927 to 0.16920, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1692 - accuracy: 0.2263 - mse: 0.1574\n","Epoch 92/200\n","593/608 [============================>.] - ETA: 0s - loss: 0.1694 - accuracy: 0.2270 - mse: 0.1573\n","Epoch 92: loss improved from 0.16920 to 0.16913, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1691 - accuracy: 0.2263 - mse: 0.1574\n","Epoch 93/200\n","598/608 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.2267 - mse: 0.1572\n","Epoch 93: loss improved from 0.16913 to 0.16906, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1691 - accuracy: 0.2263 - mse: 0.1573\n","Epoch 94/200\n","593/608 [============================>.] - ETA: 0s - loss: 0.1693 - accuracy: 0.2270 - mse: 0.1572\n","Epoch 94: loss improved from 0.16906 to 0.16899, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1690 - accuracy: 0.2263 - mse: 0.1572\n","Epoch 95/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.2271 - mse: 0.1571\n","Epoch 95: loss improved from 0.16899 to 0.16892, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1689 - accuracy: 0.2263 - mse: 0.1572\n","Epoch 96/200\n","590/608 [============================>.] - ETA: 0s - loss: 0.1684 - accuracy: 0.2276 - mse: 0.1570\n","Epoch 96: loss improved from 0.16892 to 0.16885, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1689 - accuracy: 0.2263 - mse: 0.1571\n","Epoch 97/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1690 - accuracy: 0.2268 - mse: 0.1570\n","Epoch 97: loss improved from 0.16885 to 0.16879, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1688 - accuracy: 0.2263 - mse: 0.1571\n","Epoch 98/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.2269 - mse: 0.1569\n","Epoch 98: loss improved from 0.16879 to 0.16872, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1687 - accuracy: 0.2263 - mse: 0.1570\n","Epoch 99/200\n","607/608 [============================>.] - ETA: 0s - loss: 0.1687 - accuracy: 0.2264 - mse: 0.1569\n","Epoch 99: loss improved from 0.16872 to 0.16866, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1687 - accuracy: 0.2263 - mse: 0.1569\n","Epoch 100/200\n","603/608 [============================>.] - ETA: 0s - loss: 0.1687 - accuracy: 0.2263 - mse: 0.1569\n","Epoch 100: loss improved from 0.16866 to 0.16859, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1686 - accuracy: 0.2263 - mse: 0.1569\n","Epoch 101/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.2268 - mse: 0.1568\n","Epoch 101: loss improved from 0.16859 to 0.16853, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1685 - accuracy: 0.2263 - mse: 0.1568\n","Epoch 102/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1685 - accuracy: 0.2264 - mse: 0.1568\n","Epoch 102: loss improved from 0.16853 to 0.16847, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1685 - accuracy: 0.2263 - mse: 0.1568\n","Epoch 103/200\n","592/608 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.2274 - mse: 0.1567\n","Epoch 103: loss improved from 0.16847 to 0.16841, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1684 - accuracy: 0.2263 - mse: 0.1567\n","Epoch 104/200\n","605/608 [============================>.] - ETA: 0s - loss: 0.1685 - accuracy: 0.2261 - mse: 0.1567\n","Epoch 104: loss improved from 0.16841 to 0.16835, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1684 - accuracy: 0.2263 - mse: 0.1567\n","Epoch 105/200\n","598/608 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.2267 - mse: 0.1565\n","Epoch 105: loss improved from 0.16835 to 0.16829, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1683 - accuracy: 0.2263 - mse: 0.1566\n","Epoch 106/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.2264 - mse: 0.1566\n","Epoch 106: loss improved from 0.16829 to 0.16823, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1682 - accuracy: 0.2263 - mse: 0.1566\n","Epoch 107/200\n","587/608 [===========================>..] - ETA: 0s - loss: 0.1679 - accuracy: 0.2282 - mse: 0.1564\n","Epoch 107: loss improved from 0.16823 to 0.16818, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1682 - accuracy: 0.2263 - mse: 0.1565\n","Epoch 108/200\n","601/608 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.2268 - mse: 0.1564\n","Epoch 108: loss improved from 0.16818 to 0.16812, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1681 - accuracy: 0.2263 - mse: 0.1565\n","Epoch 109/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.2271 - mse: 0.1563\n","Epoch 109: loss improved from 0.16812 to 0.16806, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1681 - accuracy: 0.2263 - mse: 0.1564\n","Epoch 110/200\n","605/608 [============================>.] - ETA: 0s - loss: 0.1681 - accuracy: 0.2261 - mse: 0.1564\n","Epoch 110: loss improved from 0.16806 to 0.16801, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1680 - accuracy: 0.2263 - mse: 0.1564\n","Epoch 111/200\n","593/608 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.2270 - mse: 0.1563\n","Epoch 111: loss improved from 0.16801 to 0.16795, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1680 - accuracy: 0.2263 - mse: 0.1563\n","Epoch 112/200\n","607/608 [============================>.] - ETA: 0s - loss: 0.1679 - accuracy: 0.2264 - mse: 0.1563\n","Epoch 112: loss improved from 0.16795 to 0.16790, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1679 - accuracy: 0.2263 - mse: 0.1563\n","Epoch 113/200\n","608/608 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.2263 - mse: 0.1563\n","Epoch 113: loss improved from 0.16790 to 0.16785, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1678 - accuracy: 0.2263 - mse: 0.1563\n","Epoch 114/200\n","601/608 [============================>.] - ETA: 0s - loss: 0.1677 - accuracy: 0.2268 - mse: 0.1561\n","Epoch 114: loss improved from 0.16785 to 0.16779, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1678 - accuracy: 0.2263 - mse: 0.1562\n","Epoch 115/200\n","587/608 [===========================>..] - ETA: 0s - loss: 0.1674 - accuracy: 0.2282 - mse: 0.1560\n","Epoch 115: loss improved from 0.16779 to 0.16774, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1677 - accuracy: 0.2263 - mse: 0.1562\n","Epoch 116/200\n","605/608 [============================>.] - ETA: 0s - loss: 0.1678 - accuracy: 0.2261 - mse: 0.1561\n","Epoch 116: loss improved from 0.16774 to 0.16769, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1677 - accuracy: 0.2263 - mse: 0.1561\n","Epoch 117/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1679 - accuracy: 0.2268 - mse: 0.1560\n","Epoch 117: loss improved from 0.16769 to 0.16764, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1676 - accuracy: 0.2263 - mse: 0.1561\n","Epoch 118/200\n","608/608 [==============================] - ETA: 0s - loss: 0.1676 - accuracy: 0.2263 - mse: 0.1560\n","Epoch 118: loss improved from 0.16764 to 0.16759, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1676 - accuracy: 0.2263 - mse: 0.1560\n","Epoch 119/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1676 - accuracy: 0.2264 - mse: 0.1560\n","Epoch 119: loss improved from 0.16759 to 0.16754, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1675 - accuracy: 0.2263 - mse: 0.1560\n","Epoch 120/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1677 - accuracy: 0.2269 - mse: 0.1559\n","Epoch 120: loss improved from 0.16754 to 0.16749, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1675 - accuracy: 0.2263 - mse: 0.1560\n","Epoch 121/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1676 - accuracy: 0.2269 - mse: 0.1559\n","Epoch 121: loss improved from 0.16749 to 0.16745, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1674 - accuracy: 0.2263 - mse: 0.1559\n","Epoch 122/200\n","607/608 [============================>.] - ETA: 0s - loss: 0.1674 - accuracy: 0.2264 - mse: 0.1559\n","Epoch 122: loss improved from 0.16745 to 0.16740, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1674 - accuracy: 0.2263 - mse: 0.1559\n","Epoch 123/200\n","585/608 [===========================>..] - ETA: 0s - loss: 0.1670 - accuracy: 0.2288 - mse: 0.1557\n","Epoch 123: loss improved from 0.16740 to 0.16735, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1674 - accuracy: 0.2263 - mse: 0.1559\n","Epoch 124/200\n","587/608 [===========================>..] - ETA: 0s - loss: 0.1670 - accuracy: 0.2282 - mse: 0.1557\n","Epoch 124: loss improved from 0.16735 to 0.16731, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1673 - accuracy: 0.2263 - mse: 0.1558\n","Epoch 125/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1673 - accuracy: 0.2264 - mse: 0.1558\n","Epoch 125: loss improved from 0.16731 to 0.16726, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1673 - accuracy: 0.2263 - mse: 0.1558\n","Epoch 126/200\n","590/608 [============================>.] - ETA: 0s - loss: 0.1667 - accuracy: 0.2276 - mse: 0.1556\n","Epoch 126: loss improved from 0.16726 to 0.16721, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1672 - accuracy: 0.2263 - mse: 0.1558\n","Epoch 127/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1672 - accuracy: 0.2264 - mse: 0.1557\n","Epoch 127: loss improved from 0.16721 to 0.16717, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1672 - accuracy: 0.2263 - mse: 0.1557\n","Epoch 128/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1673 - accuracy: 0.2269 - mse: 0.1556\n","Epoch 128: loss improved from 0.16717 to 0.16712, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1671 - accuracy: 0.2263 - mse: 0.1557\n","Epoch 129/200\n","608/608 [==============================] - ETA: 0s - loss: 0.1671 - accuracy: 0.2263 - mse: 0.1557\n","Epoch 129: loss improved from 0.16712 to 0.16708, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1671 - accuracy: 0.2263 - mse: 0.1557\n","Epoch 130/200\n","604/608 [============================>.] - ETA: 0s - loss: 0.1672 - accuracy: 0.2261 - mse: 0.1556\n","Epoch 130: loss improved from 0.16708 to 0.16704, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1670 - accuracy: 0.2263 - mse: 0.1556\n","Epoch 131/200\n","586/608 [===========================>..] - ETA: 0s - loss: 0.1666 - accuracy: 0.2286 - mse: 0.1554\n","Epoch 131: loss improved from 0.16704 to 0.16699, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1670 - accuracy: 0.2263 - mse: 0.1556\n","Epoch 132/200\n","590/608 [============================>.] - ETA: 0s - loss: 0.1665 - accuracy: 0.2276 - mse: 0.1554\n","Epoch 132: loss improved from 0.16699 to 0.16695, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1670 - accuracy: 0.2263 - mse: 0.1556\n","Epoch 133/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.2269 - mse: 0.1555\n","Epoch 133: loss improved from 0.16695 to 0.16691, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1669 - accuracy: 0.2263 - mse: 0.1555\n","Epoch 134/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1670 - accuracy: 0.2269 - mse: 0.1554\n","Epoch 134: loss improved from 0.16691 to 0.16687, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1669 - accuracy: 0.2263 - mse: 0.1555\n","Epoch 135/200\n","603/608 [============================>.] - ETA: 0s - loss: 0.1669 - accuracy: 0.2263 - mse: 0.1555\n","Epoch 135: loss improved from 0.16687 to 0.16683, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1668 - accuracy: 0.2263 - mse: 0.1555\n","Epoch 136/200\n","594/608 [============================>.] - ETA: 0s - loss: 0.1670 - accuracy: 0.2269 - mse: 0.1554\n","Epoch 136: loss improved from 0.16683 to 0.16678, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1668 - accuracy: 0.2263 - mse: 0.1555\n","Epoch 137/200\n","603/608 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.2263 - mse: 0.1554\n","Epoch 137: loss improved from 0.16678 to 0.16674, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1667 - accuracy: 0.2263 - mse: 0.1554\n","Epoch 138/200\n","597/608 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.2265 - mse: 0.1553\n","Epoch 138: loss improved from 0.16674 to 0.16670, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1667 - accuracy: 0.2263 - mse: 0.1554\n","Epoch 139/200\n","600/608 [============================>.] - ETA: 0s - loss: 0.1667 - accuracy: 0.2270 - mse: 0.1553\n","Epoch 139: loss improved from 0.16670 to 0.16666, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1667 - accuracy: 0.2263 - mse: 0.1554\n","Epoch 140/200\n","601/608 [============================>.] - ETA: 0s - loss: 0.1666 - accuracy: 0.2268 - mse: 0.1553\n","Epoch 140: loss improved from 0.16666 to 0.16662, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1666 - accuracy: 0.2263 - mse: 0.1553\n","Epoch 141/200\n","607/608 [============================>.] - ETA: 0s - loss: 0.1666 - accuracy: 0.2264 - mse: 0.1553\n","Epoch 141: loss improved from 0.16662 to 0.16658, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1666 - accuracy: 0.2263 - mse: 0.1553\n","Epoch 142/200\n","606/608 [============================>.] - ETA: 0s - loss: 0.1666 - accuracy: 0.2258 - mse: 0.1553\n","Epoch 142: loss improved from 0.16658 to 0.16655, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1665 - accuracy: 0.2263 - mse: 0.1553\n","Epoch 143/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1666 - accuracy: 0.2264 - mse: 0.1553\n","Epoch 143: loss improved from 0.16655 to 0.16651, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1665 - accuracy: 0.2263 - mse: 0.1553\n","Epoch 144/200\n","607/608 [============================>.] - ETA: 0s - loss: 0.1665 - accuracy: 0.2264 - mse: 0.1553\n","Epoch 144: loss improved from 0.16651 to 0.16647, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1665 - accuracy: 0.2263 - mse: 0.1553\n","Epoch 145/200\n","591/608 [============================>.] - ETA: 0s - loss: 0.1667 - accuracy: 0.2272 - mse: 0.1552\n","Epoch 145: loss improved from 0.16647 to 0.16643, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1664 - accuracy: 0.2263 - mse: 0.1552\n","Epoch 146/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1664 - accuracy: 0.2271 - mse: 0.1551\n","Epoch 146: loss improved from 0.16643 to 0.16639, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1664 - accuracy: 0.2263 - mse: 0.1552\n","Epoch 147/200\n","594/608 [============================>.] - ETA: 0s - loss: 0.1666 - accuracy: 0.2269 - mse: 0.1551\n","Epoch 147: loss improved from 0.16639 to 0.16636, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1664 - accuracy: 0.2263 - mse: 0.1552\n","Epoch 148/200\n","594/608 [============================>.] - ETA: 0s - loss: 0.1666 - accuracy: 0.2269 - mse: 0.1551\n","Epoch 148: loss improved from 0.16636 to 0.16632, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1663 - accuracy: 0.2263 - mse: 0.1552\n","Epoch 149/200\n","589/608 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.2279 - mse: 0.1550\n","Epoch 149: loss improved from 0.16632 to 0.16628, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1663 - accuracy: 0.2263 - mse: 0.1551\n","Epoch 150/200\n","594/608 [============================>.] - ETA: 0s - loss: 0.1665 - accuracy: 0.2269 - mse: 0.1551\n","Epoch 150: loss improved from 0.16628 to 0.16625, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1662 - accuracy: 0.2263 - mse: 0.1551\n","Epoch 151/200\n","591/608 [============================>.] - ETA: 0s - loss: 0.1665 - accuracy: 0.2272 - mse: 0.1550\n","Epoch 151: loss improved from 0.16625 to 0.16621, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1662 - accuracy: 0.2263 - mse: 0.1551\n","Epoch 152/200\n","600/608 [============================>.] - ETA: 0s - loss: 0.1662 - accuracy: 0.2270 - mse: 0.1550\n","Epoch 152: loss improved from 0.16621 to 0.16618, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1662 - accuracy: 0.2263 - mse: 0.1551\n","Epoch 153/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1662 - accuracy: 0.2264 - mse: 0.1550\n","Epoch 153: loss improved from 0.16618 to 0.16614, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1661 - accuracy: 0.2263 - mse: 0.1551\n","Epoch 154/200\n","600/608 [============================>.] - ETA: 0s - loss: 0.1661 - accuracy: 0.2270 - mse: 0.1550\n","Epoch 154: loss improved from 0.16614 to 0.16611, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1661 - accuracy: 0.2263 - mse: 0.1550\n","Epoch 155/200\n","604/608 [============================>.] - ETA: 0s - loss: 0.1663 - accuracy: 0.2261 - mse: 0.1550\n","Epoch 155: loss improved from 0.16611 to 0.16607, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1661 - accuracy: 0.2263 - mse: 0.1550\n","Epoch 156/200\n","606/608 [============================>.] - ETA: 0s - loss: 0.1661 - accuracy: 0.2258 - mse: 0.1550\n","Epoch 156: loss improved from 0.16607 to 0.16604, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1660 - accuracy: 0.2263 - mse: 0.1550\n","Epoch 157/200\n","600/608 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.2270 - mse: 0.1549\n","Epoch 157: loss improved from 0.16604 to 0.16600, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1660 - accuracy: 0.2263 - mse: 0.1550\n","Epoch 158/200\n","594/608 [============================>.] - ETA: 0s - loss: 0.1662 - accuracy: 0.2269 - mse: 0.1549\n","Epoch 158: loss improved from 0.16600 to 0.16597, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1660 - accuracy: 0.2263 - mse: 0.1550\n","Epoch 159/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1659 - accuracy: 0.2271 - mse: 0.1549\n","Epoch 159: loss improved from 0.16597 to 0.16593, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1659 - accuracy: 0.2263 - mse: 0.1549\n","Epoch 160/200\n","592/608 [============================>.] - ETA: 0s - loss: 0.1663 - accuracy: 0.2274 - mse: 0.1549\n","Epoch 160: loss improved from 0.16593 to 0.16590, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1659 - accuracy: 0.2263 - mse: 0.1549\n","Epoch 161/200\n","584/608 [===========================>..] - ETA: 0s - loss: 0.1656 - accuracy: 0.2292 - mse: 0.1548\n","Epoch 161: loss improved from 0.16590 to 0.16587, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1659 - accuracy: 0.2263 - mse: 0.1549\n","Epoch 162/200\n","590/608 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.2276 - mse: 0.1548\n","Epoch 162: loss improved from 0.16587 to 0.16583, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1658 - accuracy: 0.2263 - mse: 0.1549\n","Epoch 163/200\n","605/608 [============================>.] - ETA: 0s - loss: 0.1659 - accuracy: 0.2261 - mse: 0.1549\n","Epoch 163: loss improved from 0.16583 to 0.16580, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1658 - accuracy: 0.2263 - mse: 0.1549\n","Epoch 164/200\n","587/608 [===========================>..] - ETA: 0s - loss: 0.1655 - accuracy: 0.2282 - mse: 0.1548\n","Epoch 164: loss improved from 0.16580 to 0.16577, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1658 - accuracy: 0.2263 - mse: 0.1549\n","Epoch 165/200\n","595/608 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.2268 - mse: 0.1548\n","Epoch 165: loss improved from 0.16577 to 0.16574, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1657 - accuracy: 0.2263 - mse: 0.1548\n","Epoch 166/200\n","597/608 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.2265 - mse: 0.1548\n","Epoch 166: loss improved from 0.16574 to 0.16570, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1657 - accuracy: 0.2263 - mse: 0.1548\n","Epoch 167/200\n","590/608 [============================>.] - ETA: 0s - loss: 0.1652 - accuracy: 0.2276 - mse: 0.1547\n","Epoch 167: loss improved from 0.16570 to 0.16567, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1657 - accuracy: 0.2263 - mse: 0.1548\n","Epoch 168/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1657 - accuracy: 0.2264 - mse: 0.1548\n","Epoch 168: loss improved from 0.16567 to 0.16564, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1656 - accuracy: 0.2263 - mse: 0.1548\n","Epoch 169/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.2269 - mse: 0.1547\n","Epoch 169: loss improved from 0.16564 to 0.16561, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1656 - accuracy: 0.2263 - mse: 0.1548\n","Epoch 170/200\n","589/608 [============================>.] - ETA: 0s - loss: 0.1653 - accuracy: 0.2279 - mse: 0.1547\n","Epoch 170: loss improved from 0.16561 to 0.16558, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1656 - accuracy: 0.2263 - mse: 0.1548\n","Epoch 171/200\n","584/608 [===========================>..] - ETA: 0s - loss: 0.1653 - accuracy: 0.2292 - mse: 0.1546\n","Epoch 171: loss improved from 0.16558 to 0.16555, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1655 - accuracy: 0.2263 - mse: 0.1548\n","Epoch 172/200\n","588/608 [============================>.] - ETA: 0s - loss: 0.1652 - accuracy: 0.2279 - mse: 0.1546\n","Epoch 172: loss improved from 0.16555 to 0.16552, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1655 - accuracy: 0.2263 - mse: 0.1547\n","Epoch 173/200\n","605/608 [============================>.] - ETA: 0s - loss: 0.1656 - accuracy: 0.2261 - mse: 0.1547\n","Epoch 173: loss improved from 0.16552 to 0.16549, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1655 - accuracy: 0.2263 - mse: 0.1547\n","Epoch 174/200\n","606/608 [============================>.] - ETA: 0s - loss: 0.1655 - accuracy: 0.2258 - mse: 0.1547\n","Epoch 174: loss improved from 0.16549 to 0.16546, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1655 - accuracy: 0.2263 - mse: 0.1547\n","Epoch 175/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.2271 - mse: 0.1546\n","Epoch 175: loss improved from 0.16546 to 0.16543, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1654 - accuracy: 0.2263 - mse: 0.1547\n","Epoch 176/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1656 - accuracy: 0.2269 - mse: 0.1546\n","Epoch 176: loss improved from 0.16543 to 0.16540, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1654 - accuracy: 0.2263 - mse: 0.1547\n","Epoch 177/200\n","598/608 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.2267 - mse: 0.1546\n","Epoch 177: loss improved from 0.16540 to 0.16537, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1654 - accuracy: 0.2263 - mse: 0.1547\n","Epoch 178/200\n","607/608 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.2264 - mse: 0.1547\n","Epoch 178: loss improved from 0.16537 to 0.16534, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1653 - accuracy: 0.2263 - mse: 0.1547\n","Epoch 179/200\n","597/608 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.2265 - mse: 0.1546\n","Epoch 179: loss improved from 0.16534 to 0.16531, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1653 - accuracy: 0.2263 - mse: 0.1546\n","Epoch 180/200\n","598/608 [============================>.] - ETA: 0s - loss: 0.1653 - accuracy: 0.2267 - mse: 0.1546\n","Epoch 180: loss improved from 0.16531 to 0.16528, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1653 - accuracy: 0.2263 - mse: 0.1546\n","Epoch 181/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.2269 - mse: 0.1546\n","Epoch 181: loss improved from 0.16528 to 0.16525, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1652 - accuracy: 0.2263 - mse: 0.1546\n","Epoch 182/200\n","586/608 [===========================>..] - ETA: 0s - loss: 0.1649 - accuracy: 0.2286 - mse: 0.1545\n","Epoch 182: loss improved from 0.16525 to 0.16522, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1652 - accuracy: 0.2263 - mse: 0.1546\n","Epoch 183/200\n","594/608 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.2269 - mse: 0.1546\n","Epoch 183: loss improved from 0.16522 to 0.16519, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1652 - accuracy: 0.2263 - mse: 0.1546\n","Epoch 184/200\n","601/608 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.2268 - mse: 0.1545\n","Epoch 184: loss improved from 0.16519 to 0.16516, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1652 - accuracy: 0.2263 - mse: 0.1546\n","Epoch 185/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.2271 - mse: 0.1545\n","Epoch 185: loss improved from 0.16516 to 0.16513, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1651 - accuracy: 0.2263 - mse: 0.1546\n","Epoch 186/200\n","591/608 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.2272 - mse: 0.1545\n","Epoch 186: loss improved from 0.16513 to 0.16510, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1651 - accuracy: 0.2263 - mse: 0.1546\n","Epoch 187/200\n","590/608 [============================>.] - ETA: 0s - loss: 0.1646 - accuracy: 0.2276 - mse: 0.1544\n","Epoch 187: loss improved from 0.16510 to 0.16508, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1651 - accuracy: 0.2263 - mse: 0.1546\n","Epoch 188/200\n","592/608 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.2274 - mse: 0.1545\n","Epoch 188: loss improved from 0.16508 to 0.16505, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1650 - accuracy: 0.2263 - mse: 0.1546\n","Epoch 189/200\n","607/608 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.2264 - mse: 0.1545\n","Epoch 189: loss improved from 0.16505 to 0.16502, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1650 - accuracy: 0.2263 - mse: 0.1545\n","Epoch 190/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.2264 - mse: 0.1545\n","Epoch 190: loss improved from 0.16502 to 0.16499, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1650 - accuracy: 0.2263 - mse: 0.1545\n","Epoch 191/200\n","602/608 [============================>.] - ETA: 0s - loss: 0.1650 - accuracy: 0.2264 - mse: 0.1545\n","Epoch 191: loss improved from 0.16499 to 0.16497, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1650 - accuracy: 0.2263 - mse: 0.1545\n","Epoch 192/200\n","605/608 [============================>.] - ETA: 0s - loss: 0.1650 - accuracy: 0.2261 - mse: 0.1545\n","Epoch 192: loss improved from 0.16497 to 0.16494, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1649 - accuracy: 0.2263 - mse: 0.1545\n","Epoch 193/200\n","601/608 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.2268 - mse: 0.1544\n","Epoch 193: loss improved from 0.16494 to 0.16491, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1649 - accuracy: 0.2263 - mse: 0.1545\n","Epoch 194/200\n","601/608 [============================>.] - ETA: 0s - loss: 0.1648 - accuracy: 0.2268 - mse: 0.1544\n","Epoch 194: loss improved from 0.16491 to 0.16488, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1649 - accuracy: 0.2263 - mse: 0.1545\n","Epoch 195/200\n","599/608 [============================>.] - ETA: 0s - loss: 0.1648 - accuracy: 0.2271 - mse: 0.1544\n","Epoch 195: loss improved from 0.16488 to 0.16486, saving model to pesos.h5\n","608/608 [==============================] - 2s 3ms/step - loss: 0.1649 - accuracy: 0.2263 - mse: 0.1545\n","Epoch 196/200\n","587/608 [===========================>..] - ETA: 0s - loss: 0.1646 - accuracy: 0.2282 - mse: 0.1544\n","Epoch 196: loss improved from 0.16486 to 0.16483, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1648 - accuracy: 0.2263 - mse: 0.1545\n","Epoch 197/200\n","585/608 [===========================>..] - ETA: 0s - loss: 0.1645 - accuracy: 0.2288 - mse: 0.1544\n","Epoch 197: loss improved from 0.16483 to 0.16480, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1648 - accuracy: 0.2263 - mse: 0.1545\n","Epoch 198/200\n","587/608 [===========================>..] - ETA: 0s - loss: 0.1645 - accuracy: 0.2282 - mse: 0.1544\n","Epoch 198: loss improved from 0.16480 to 0.16478, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1648 - accuracy: 0.2263 - mse: 0.1545\n","Epoch 199/200\n","601/608 [============================>.] - ETA: 0s - loss: 0.1647 - accuracy: 0.2268 - mse: 0.1544\n","Epoch 199: loss improved from 0.16478 to 0.16475, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1648 - accuracy: 0.2263 - mse: 0.1545\n","Epoch 200/200\n","596/608 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.2269 - mse: 0.1544\n","Epoch 200: loss improved from 0.16475 to 0.16472, saving model to pesos.h5\n","608/608 [==============================] - 1s 2ms/step - loss: 0.1647 - accuracy: 0.2263 - mse: 0.1545\n"]}]},{"cell_type":"markdown","metadata":{"id":"p3QoaI9fhEpu"},"source":["## Predicción sobre test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2900,"status":"ok","timestamp":1702672026344,"user":{"displayName":"LUCAS GALLEGO BRAVO","userId":"17964449862452331851"},"user_tz":-60},"id":"IBIEfgaqgJZt","outputId":"3984ee55-162e-44e1-b0fc-520d98a23c51"},"outputs":[{"output_type":"stream","name":"stdout","text":["304/304 [==============================] - 1s 2ms/step\n","304/304 [==============================] - 1s 2ms/step - loss: 0.1550 - accuracy: 0.2224 - mse: 0.1550\n","Recall: 0.2224\n","Precisión: 0.0495\n","F1-Score: 0.0809\n","Matriz de Confusión: \n","\n","[[   0    0 3517    0    0]\n"," [   0    0 1925    0    0]\n"," [   0    0 2161    0    0]\n"," [   0    0 1478    0    0]\n"," [   0    0  636    0    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["# Carga los mejores pesos\n","model.load_weights('pesos.h5')\n","\n","# Hacer predicciones en el conjunto de prueba\n","y_pred = model.predict(X_test)\n","\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","# Convertir las etiquetas reales a clases (índice de la clase con valor 1 en la codificación one-hot)\n","y_true_classes = np.argmax(y_test_encoded, axis=1)\n","\n","# Evaluamos el modelo para sacar el MSE y accuracy\n","evaluate =  model.evaluate(X_test, y_test_encoded)\n","\n","# Calcular y mostrar las métricas de evaluación\n","accuracy = evaluate[1]\n","recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n","precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n","f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n","confusion_mat = confusion_matrix(y_true_classes, y_pred_classes)\n","mse_test = evaluate[2]  # El índice 2 corresponde a la métrica 'mse' en la lista de métricas\n","\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"Precisión: {precision:.4f}\")\n","print(f\"F1-Score: {f1:.4f}\")\n","print(\"Matriz de Confusión: \\n\")\n","print(confusion_mat)"]},{"cell_type":"markdown","source":["## Guardar la predicción"],"metadata":{"id":"GaxGPvRcOHUq"}},{"cell_type":"code","source":["resultados_test = pd.DataFrame(columns=[\"Target\",\"Predicciones\",\"Clase predicha\"])\n","\n","i = 0\n","while i < 9717:\n","  var1 = y_pred[i]\n","  var2 = y_test.loc[i,\"label\"] #solo queremos la clase\n","  clase_predicha = np.argmax(var1)\n","  resultados_test.loc[i] = [var2,var1,clase_predicha]\n","  i = i + 1\n","\n","# Una vez se tiene esto hecho, se puede guardar como un fichero:\n","resultados_test.to_csv('resultados_test.csv', index=False)\n","archivo_txt = 'resultados_test.txt'\n","\n","# Escribe los datos en el archivo de texto\n","resultados_test.to_csv(archivo_txt, index=False, sep='\\t')  # 'sep' es el delimitador, aquí se usa tabulación"],"metadata":{"id":"1JRiTPdzOJx4"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNreAJdlYbFo+kUSefcnrTz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}